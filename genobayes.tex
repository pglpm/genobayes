\pdfoutput=1
%% Author: PGL  Porta Mana
%% Created: 2018-08-07T20:36:31+0200
%% Last-Updated: 2018-11-11T22:47:06+0100
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Report-no: ***
\newif\ifarxiv
\arxivfalse
\ifarxiv\pdfmapfile{+classico.map}\fi
\newif\ifafour
\afourfalse % true = A4, false = A5
\newif\iftypodisclaim % typographical disclaim on the side
\typodisclaimtrue
\newif\ifpublic
\publictrue % true = for publication, false = personal notes
\newcommand*{\memfontfamily}{zplx}
\newcommand*{\memfontpack}{newpxtext}
\documentclass[\ifafour a4paper,12pt,\else a5paper,10pt,\fi%extrafontsizes,%
onecolumn,oneside,article,%french,italian,german,swedish,latin,
british%
]{memoir}
\newcommand*{\updated}{\today}
\newcommand*{\firstdraft}{22 August 2018}
\newcommand*{\firstpublished}{***}
\newcommand*{\propertitle}{Do genes keep us awake?\\\large Research notes}
\newcommand*{\pdftitle}{Do genes keep us awake? -- Research notes}
\newcommand*{\headtitle}{Insomnia and genetic data}
\newcommand*{\pdfauthor}{D. Bragantini, I. C. G\"uzey, P.G.L.  Porta Mana, Y. Roudi}
\newcommand*{\headauthor}{\ifpublic C\"uneyt, Daniela, Luca, Yasser%
\else Luca\fi}
\newcommand*{\reporthead}{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{pifont}
%\usepackage{fontawesome}
\usepackage[T1]{fontenc} 
\input{glyphtounicode} \pdfgentounicode=1
\usepackage[utf8]{inputenx}
%\usepackage{newunicodechar}
% \newunicodechar{Ĕ}{\u{E}}
% \newunicodechar{ĕ}{\u{e}}
% \newunicodechar{Ĭ}{\u{I}}
% \newunicodechar{ĭ}{\u{\i}}
% \newunicodechar{Ŏ}{\u{O}}
% \newunicodechar{ŏ}{\u{o}}
% \newunicodechar{Ŭ}{\u{U}}
% \newunicodechar{ŭ}{\u{u}}
% \newunicodechar{Ā}{\=A}
% \newunicodechar{ā}{\=a}
% \newunicodechar{Ē}{\=E}
% \newunicodechar{ē}{\=e}
% \newunicodechar{Ī}{\=I}
% \newunicodechar{ī}{\={\i}}
% \newunicodechar{Ō}{\=O}
% \newunicodechar{ō}{\=o}
% \newunicodechar{Ū}{\=U}
% \newunicodechar{ū}{\=u}
% \newunicodechar{Ȳ}{\=Y}
% \newunicodechar{ȳ}{\=y}

\newcommand*{\bmmax}{0} % reduce number of bold fonts, before bm
\newcommand*{\hmmax}{0} % reduce number of heavy fonts, before bm
\usepackage{textcomp}
\usepackage[normalem]{ulem}
% \makeatletter
% \def\ssout{\bgroup \ULdepth=-.35ex%\UL@setULdepth
%  \markoverwith{\lower\ULdepth\hbox
%    {\kern-.03em\vbox{\hrule width.2em\kern1.2\p@\hrule}\kern-.03em}}%
%  \ULon}
% \makeatother
\usepackage{amsmath}
\usepackage{mathtools}
\addtolength{\jot}{\jot} % increase spacing in multiline formulae
\usepackage{empheq}% automatically calls amsmath and mathtools
\newcommand*{\widefbox}[1]{\fbox{\hspace{1em}#1\hspace{1em}}}
\setlength{\multlinegap}{0pt}
%\usepackage{fancybox}
%\usepackage{framed}
% \usepackage[misc]{ifsym} % for dice
% \newcommand*{\diceone}{{\scriptsize\Cube{1}}}
\usepackage{amssymb}
\usepackage{amsxtra}

\usepackage[main=british,french,italian,german,swedish,latin,esperanto]{babel}\selectlanguage{british}
\newcommand*{\langfrench}{\foreignlanguage{french}}
\newcommand*{\langgerman}{\foreignlanguage{german}}
\newcommand*{\langitalian}{\foreignlanguage{italian}}
\newcommand*{\langswedish}{\foreignlanguage{swedish}}
\newcommand*{\langlatin}{\foreignlanguage{latin}}
\newcommand*{\langnohyph}{\foreignlanguage{nohyphenation}}

\usepackage[autostyle=false,autopunct=false,english=british]{csquotes}
\setquotestyle{british}

\usepackage{amsthm}
\newcommand*{\QED}{\textsc{q.e.d.}}
\renewcommand*{\qedsymbol}{\QED}
\theoremstyle{remark}
\newtheorem{note}{Note}
\newtheorem*{remark}{Note}
\newtheoremstyle{innote}{\parsep}{\parsep}{\footnotesize}{}{}{}{0pt}{}
\theoremstyle{innote}
\newtheorem*{innote}{}

\usepackage[shortlabels,inline]{enumitem}
\SetEnumitemKey{para}{itemindent=\parindent,leftmargin=0pt,listparindent=\parindent,parsep=0pt,itemsep=\topsep}
% \begin{asparaenum} = \begin{enumerate}[para]
% \begin{inparaenum} = \begin{enumerate*}
\setlist[enumerate,2]{label=\alph*.}
\setlist[enumerate]{label=\arabic*.,leftmargin=1.5\parindent}
\setlist[itemize]{leftmargin=1.5\parindent}
\setlist[description]{leftmargin=1.5\parindent}

\usepackage[babel,theoremfont]{newpxtext}
\usepackage[bigdelims,nosymbolsc%,smallerops % probably arXiv doesn't have it
]{newpxmath}
\useosf\linespread{1.083}
%% smaller operators for old version of newpxmath
\makeatletter
\def\re@DeclareMathSymbol#1#2#3#4{%
    \let#1=\undefined
    \DeclareMathSymbol{#1}{#2}{#3}{#4}}
%\re@DeclareMathSymbol{\bigsqcupop}{\mathop}{largesymbols}{"46}
%\re@DeclareMathSymbol{\bigodotop}{\mathop}{largesymbols}{"4A}
\re@DeclareMathSymbol{\bigoplusop}{\mathop}{largesymbols}{"4C}
\re@DeclareMathSymbol{\bigotimesop}{\mathop}{largesymbols}{"4E}
\re@DeclareMathSymbol{\sumop}{\mathop}{largesymbols}{"50}
\re@DeclareMathSymbol{\prodop}{\mathop}{largesymbols}{"51}
\re@DeclareMathSymbol{\bigcupop}{\mathop}{largesymbols}{"53}
\re@DeclareMathSymbol{\bigcapop}{\mathop}{largesymbols}{"54}
%\re@DeclareMathSymbol{\biguplusop}{\mathop}{largesymbols}{"55}
\re@DeclareMathSymbol{\bigwedgeop}{\mathop}{largesymbols}{"56}
\re@DeclareMathSymbol{\bigveeop}{\mathop}{largesymbols}{"57}
%\re@DeclareMathSymbol{\bigcupdotop}{\mathop}{largesymbols}{"DF}
%\re@DeclareMathSymbol{\bigcapplusop}{\mathop}{largesymbolsPXA}{"00}
%\re@DeclareMathSymbol{\bigsqcupplusop}{\mathop}{largesymbolsPXA}{"02}
%\re@DeclareMathSymbol{\bigsqcapplusop}{\mathop}{largesymbolsPXA}{"04}
%\re@DeclareMathSymbol{\bigsqcapop}{\mathop}{largesymbolsPXA}{"06}
\re@DeclareMathSymbol{\bigtimesop}{\mathop}{largesymbolsPXA}{"10}
%\re@DeclareMathSymbol{\coprodop}{\mathop}{largesymbols}{"60}
%\re@DeclareMathSymbol{\varprod}{\mathop}{largesymbolsPXA}{16}
\makeatother


%% With euler font cursive for Greek letters - the [1] means 100% scaling
\DeclareFontFamily{U}{egreek}{\skewchar\font'177}%
\DeclareFontShape{U}{egreek}{m}{n}{<-6>s*[1]eurm5 <6-8>s*[1]eurm7 <8->s*[1]eurm10}{}%
\DeclareFontShape{U}{egreek}{m}{it}{<->s*[1]eurmo10}{}%
\DeclareFontShape{U}{egreek}{b}{n}{<-6>s*[1]eurb5 <6-8>s*[1]eurb7 <8->s*[1]eurb10}{}%
\DeclareFontShape{U}{egreek}{b}{it}{<->s*[1]eurbo10}{}%
\DeclareSymbolFont{egreeki}{U}{egreek}{m}{it}%
\SetSymbolFont{egreeki}{bold}{U}{egreek}{b}{it}% from the amsfonts package
\DeclareSymbolFont{egreekr}{U}{egreek}{m}{n}%
\SetSymbolFont{egreekr}{bold}{U}{egreek}{b}{n}% from the amsfonts package
% Take also \sum, \prod, \coprod symbols from Euler fonts
\DeclareFontFamily{U}{egreekx}{\skewchar\font'177}
\DeclareFontShape{U}{egreekx}{m}{n}{%
       <-7.5>s*[0.9]euex7%
    <7.5-8.5>s*[0.9]euex8%
    <8.5-9.5>s*[0.9]euex9%
    <9.5->s*[0.9]euex10%
}{}
\DeclareSymbolFont{egreekx}{U}{egreekx}{m}{n}
\DeclareMathSymbol{\sumop}{\mathop}{egreekx}{"50}
\DeclareMathSymbol{\prodop}{\mathop}{egreekx}{"51}
\DeclareMathSymbol{\coprodop}{\mathop}{egreekx}{"60}
\makeatletter
\def\sum{\DOTSI\sumop\slimits@}
\def\prod{\DOTSI\prodop\slimits@}
\def\coprod{\DOTSI\coprodop\slimits@}
\makeatother
\ifarxiv\else\input{undefinegreek.tex}\fi% make sure no CMF greek letters sneak in
% Greek letters not usually given in LaTeX. Comment the unneeded ones
% \DeclareMathSymbol{\varpartial}{\mathalpha}{egreeki}{"40}
 \DeclareMathSymbol{\partialup}{\mathalpha}{egreekr}{"40}
% \DeclareMathSymbol{\alpha}{\mathalpha}{egreeki}{"0B}
% \DeclareMathSymbol{\beta}{\mathalpha}{egreeki}{"0C}
 \DeclareMathSymbol{\gamma}{\mathalpha}{egreeki}{"0D}
% \DeclareMathSymbol{\delta}{\mathalpha}{egreeki}{"0E}
 \DeclareMathSymbol{\epsilon}{\mathalpha}{egreeki}{"0F}
% \DeclareMathSymbol{\zeta}{\mathalpha}{egreeki}{"10}
% \DeclareMathSymbol{\eta}{\mathalpha}{egreeki}{"11}
 \DeclareMathSymbol{\theta}{\mathalpha}{egreeki}{"12}
% \DeclareMathSymbol{\iota}{\mathalpha}{egreeki}{"13}
 \DeclareMathSymbol{\kappa}{\mathalpha}{egreeki}{"14}
 \DeclareMathSymbol{\lambda}{\mathalpha}{egreeki}{"15}
% \DeclareMathSymbol{\mu}{\mathalpha}{egreeki}{"16}
 \DeclareMathSymbol{\nu}{\mathalpha}{egreeki}{"17}
 \DeclareMathSymbol{\xi}{\mathalpha}{egreeki}{"18}
% \DeclareMathSymbol{\omicron}{\mathalpha}{egreeki}{"6F}
 \DeclareMathSymbol{\pi}{\mathalpha}{egreeki}{"19}
% \DeclareMathSymbol{\rho}{\mathalpha}{egreeki}{"1A}
 \DeclareMathSymbol{\sigma}{\mathalpha}{egreeki}{"1B}
% \DeclareMathSymbol{\tau}{\mathalpha}{egreeki}{"1C}
% \DeclareMathSymbol{\upsilon}{\mathalpha}{egreeki}{"1D}
% \DeclareMathSymbol{\phi}{\mathalpha}{egreeki}{"1E}
% \DeclareMathSymbol{\chi}{\mathalpha}{egreeki}{"1F}
% \DeclareMathSymbol{\psi}{\mathalpha}{egreeki}{"20}
% \DeclareMathSymbol{\omega}{\mathalpha}{egreeki}{"21}
% \DeclareMathSymbol{\varepsilon}{\mathalpha}{egreeki}{"22}
% \DeclareMathSymbol{\vartheta}{\mathalpha}{egreeki}{"23}
% \DeclareMathSymbol{\varpi}{\mathalpha}{egreeki}{"24}
% \let\varrho\rho 
% \let\varsigma\sigma
 \let\varkappa\kappa
% \DeclareMathSymbol{\varphi}{\mathalpha}{egreeki}{"27}
% %
% \DeclareMathSymbol{\varAlpha}{\mathalpha}{egreeki}{"41}
% \DeclareMathSymbol{\varBeta}{\mathalpha}{egreeki}{"42}
% \DeclareMathSymbol{\varGamma}{\mathalpha}{egreeki}{"00}
 \DeclareMathSymbol{\varDelta}{\mathalpha}{egreeki}{"01}
 \DeclareMathSymbol{\varEpsilon}{\mathalpha}{egreeki}{"45}
% \DeclareMathSymbol{\varZeta}{\mathalpha}{egreeki}{"5A}
% \DeclareMathSymbol{\varEta}{\mathalpha}{egreeki}{"48}
 \DeclareMathSymbol{\varTheta}{\mathalpha}{egreeki}{"02}
 \DeclareMathSymbol{\varIota}{\mathalpha}{egreeki}{"49}
% \DeclareMathSymbol{\varKappa}{\mathalpha}{egreeki}{"4B}
% \DeclareMathSymbol{\varLambda}{\mathalpha}{egreeki}{"03}
% \DeclareMathSymbol{\varMu}{\mathalpha}{egreeki}{"4D}
% \DeclareMathSymbol{\varNu}{\mathalpha}{egreeki}{"4E}
% \DeclareMathSymbol{\varXi}{\mathalpha}{egreeki}{"04}
% \DeclareMathSymbol{\varOmicron}{\mathalpha}{egreeki}{"4F}
 \DeclareMathSymbol{\varPi}{\mathalpha}{egreeki}{"05}
% \DeclareMathSymbol{\varRho}{\mathalpha}{egreeki}{"50}
% \DeclareMathSymbol{\varSigma}{\mathalpha}{egreeki}{"06}
% \DeclareMathSymbol{\varTau}{\mathalpha}{egreeki}{"54}
% \DeclareMathSymbol{\varUpsilon}{\mathalpha}{egreeki}{"07}
% \DeclareMathSymbol{\varPhi}{\mathalpha}{egreeki}{"08}
% \DeclareMathSymbol{\varChi}{\mathalpha}{egreeki}{"58}
% \DeclareMathSymbol{\varPsi}{\mathalpha}{egreeki}{"09}
% \DeclareMathSymbol{\varOmega}{\mathalpha}{egreeki}{"0A} 
% %
% \DeclareMathSymbol{\Alpha}{\mathalpha}{egreekr}{"41}
% \DeclareMathSymbol{\Beta}{\mathalpha}{egreekr}{"42}
 \DeclareMathSymbol{\Gamma}{\mathalpha}{egreekr}{"00}
% \DeclareMathSymbol{\Delta}{\mathalpha}{egreekr}{"01}
% \DeclareMathSymbol{\Epsilon}{\mathalpha}{egreekr}{"45}
% \DeclareMathSymbol{\Zeta}{\mathalpha}{egreekr}{"5A}
% \DeclareMathSymbol{\Eta}{\mathalpha}{egreekr}{"48}
% \DeclareMathSymbol{\Theta}{\mathalpha}{egreekr}{"02}
% \DeclareMathSymbol{\Iota}{\mathalpha}{egreekr}{"49}
% \DeclareMathSymbol{\Kappa}{\mathalpha}{egreekr}{"4B}
% \DeclareMathSymbol{\Lambda}{\mathalpha}{egreekr}{"03}
% \DeclareMathSymbol{\Mu}{\mathalpha}{egreekr}{"4D}
% \DeclareMathSymbol{\Nu}{\mathalpha}{egreekr}{"4E}
% \DeclareMathSymbol{\Xi}{\mathalpha}{egreekr}{"04}
% \DeclareMathSymbol{\Omicron}{\mathalpha}{egreekr}{"4F}
% \DeclareMathSymbol{\Pi}{\mathalpha}{egreekr}{"05}
% \DeclareMathSymbol{\Rho}{\mathalpha}{egreekr}{"50}
% \DeclareMathSymbol{\Sigma}{\mathalpha}{egreekr}{"06}
% \DeclareMathSymbol{\Tau}{\mathalpha}{egreekr}{"54}
% \DeclareMathSymbol{\Upsilon}{\mathalpha}{egreekr}{"07}
% \DeclareMathSymbol{\Phi}{\mathalpha}{egreekr}{"08}
% \DeclareMathSymbol{\Chi}{\mathalpha}{egreekr}{"58}
% \DeclareMathSymbol{\Psi}{\mathalpha}{egreekr}{"09}
% \DeclareMathSymbol{\Omega}{\mathalpha}{egreekr}{"0A}
% %
 \DeclareMathSymbol{\alphaup}{\mathalpha}{egreekr}{"0B}
 \DeclareMathSymbol{\betaup}{\mathalpha}{egreekr}{"0C}
 \DeclareMathSymbol{\gammaup}{\mathalpha}{egreekr}{"0D}
 \DeclareMathSymbol{\deltaup}{\mathalpha}{egreekr}{"0E}
% \DeclareMathSymbol{\epsilonup}{\mathalpha}{egreekr}{"0F}
% \DeclareMathSymbol{\zetaup}{\mathalpha}{egreekr}{"10}
% \DeclareMathSymbol{\etaup}{\mathalpha}{egreekr}{"11}
% \DeclareMathSymbol{\thetaup}{\mathalpha}{egreekr}{"12}
% \DeclareMathSymbol{\iotaup}{\mathalpha}{egreekr}{"13}
% \DeclareMathSymbol{\kappaup}{\mathalpha}{egreekr}{"14}
% \DeclareMathSymbol{\lambdaup}{\mathalpha}{egreekr}{"15}
% \DeclareMathSymbol{\muup}{\mathalpha}{egreekr}{"16}
% \DeclareMathSymbol{\nuup}{\mathalpha}{egreekr}{"17}
% \DeclareMathSymbol{\xiup}{\mathalpha}{egreekr}{"18}
% \DeclareMathSymbol{\omicronup}{\mathalpha}{egreekr}{"6F}
  \DeclareMathSymbol{\piup}{\mathalpha}{egreekr}{"19}
% \DeclareMathSymbol{\rhoup}{\mathalpha}{egreekr}{"1A}
% \DeclareMathSymbol{\sigmaup}{\mathalpha}{egreekr}{"1B}
% \DeclareMathSymbol{\tauup}{\mathalpha}{egreekr}{"1C}
% \DeclareMathSymbol{\upsilonup}{\mathalpha}{egreekr}{"1D}
% \DeclareMathSymbol{\phiup}{\mathalpha}{egreekr}{"1E}
% \DeclareMathSymbol{\chiup}{\mathalpha}{egreekr}{"1F}
% \DeclareMathSymbol{\psiup}{\mathalpha}{egreekr}{"20}
% \DeclareMathSymbol{\omegaup}{\mathalpha}{egreekr}{"21}
% \DeclareMathSymbol{\varepsilonup}{\mathalpha}{egreekr}{"22}
% \DeclareMathSymbol{\varthetaup}{\mathalpha}{egreekr}{"23}
% \DeclareMathSymbol{\varpiup}{\mathalpha}{egreekr}{"24}
% \let\varrhoup\rhoup 
% \let\varsigmaup\sigmaup
% \let\varkappaup\kappaup
% \DeclareMathSymbol{\varphiup}{\mathalpha}{egreekr}{"27}

% Optima as sans-serif font
%\usepackage%[scaled=0.9]%
%{classico}
\usepackage{classico} % \renewcommand\sfdefault{URWClassico-TLF}
\DeclareMathAlphabet{\mathsf}  {T1}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsf}{bold}{T1}{\sfdefault}{b}{sl}
\newcommand*{\mathte}[1]{\textbf{\textit{\textsf{#1}}}}
% Upright sans-serif math alphabet
% \DeclareMathAlphabet{\mathsu}  {T1}{\sfdefault}{m}{n}
% \SetMathAlphabet{\mathsu}{bold}{T1}{\sfdefault}{b}{n}

% DejaVu Mono as typewriter text
\usepackage[scaled=0.84]{DejaVuSansMono}


\usepackage{mathdots}

\usepackage[usenames]{xcolor}
% Tol (2012) colour-blind-, print-, screen-friendly colours, alternative scheme; Munsell terminology
\definecolor{mypurpleblue}{RGB}{68,119,170}
\definecolor{myblue}{RGB}{102,204,238}
\definecolor{mygreen}{RGB}{34,136,51}
\definecolor{myyellow}{RGB}{204,187,68}
\definecolor{myred}{RGB}{238,102,119}
\definecolor{myredpurple}{RGB}{170,51,119}
\definecolor{mygrey}{RGB}{187,187,187}

% Tol (2012) colour-blind-, print-, screen-friendly colours; Munsell terminology
% \definecolor{lbpurple}{RGB}{51,34,136}
% \definecolor{lblue}{RGB}{136,204,238}
% \definecolor{lbgreen}{RGB}{68,170,153}
% \definecolor{lgreen}{RGB}{17,119,51}
% \definecolor{lgyellow}{RGB}{153,153,51}
% \definecolor{lyellow}{RGB}{221,204,119}
% \definecolor{lred}{RGB}{204,102,119}
% \definecolor{lpred}{RGB}{136,34,85}
% \definecolor{lrpurple}{RGB}{170,68,153}
 \definecolor{lgrey}{RGB}{221,221,221}
%\newcommand*\mycolourbox[1]{%
%\colorbox{mygrey}{\hspace{1em}#1\hspace{1em}}}
\colorlet{shadecolor}{lgrey}

\usepackage{bm}
\usepackage{microtype}

\usepackage[backend=biber,mcite,%subentry,
citestyle=authoryear-comp,bibstyle=pglpm-authoryear,autopunct=false,sorting=ny,sortcites=false,natbib=false,maxcitenames=1,maxbibnames=8,minbibnames=8,giveninits=true,uniquename=false,uniquelist=false,maxalphanames=1,block=space,hyperref=true,defernumbers=false,useprefix=true,sortupper=false,language=british,parentracker=false]{biblatex}
\DeclareSortingScheme{ny}{\sort{\field{sortname}\field{author}\field{editor}}\sort{\field{year}}}
\iffalse\makeatletter%%% replace parenthesis with brackets
\newrobustcmd*{\parentexttrack}[1]{%
  \begingroup
  \blx@blxinit
  \blx@setsfcodes
  \blx@bibopenparen#1\blx@bibcloseparen
  \endgroup}
\AtEveryCite{%
  \let\parentext=\parentexttrack%
  \let\bibopenparen=\bibopenbracket%
  \let\bibcloseparen=\bibclosebracket}
\makeatother\fi
\DefineBibliographyExtras{british}{\def\finalandcomma{\addcomma}}
\renewcommand*{\finalnamedelim}{\addcomma\space}
\setcounter{biburlnumpenalty}{1}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{1}
\DeclareDelimFormat{multicitedelim}{\addsemicolon\space}
\DeclareDelimFormat{compcitedelim}{\addsemicolon\space}
\DeclareDelimFormat{postnotedelim}{\space}
\ifarxiv\else\addbibresource{portamanabib.bib}\fi
\renewcommand{\bibfont}{\footnotesize}
%\appto{\citesetup}{\footnotesize}% smaller font for citations
\defbibheading{bibliography}[\bibname]{\section*{#1}\addcontentsline{toc}{section}{#1}%\markboth{#1}{#1}
}
\newcommand*{\citep}{\parencites}
\newcommand*{\citey}{\parencites*}
%\renewcommand*{\cite}{\parencite}
\renewcommand*{\cites}{\parencites}
\providecommand{\href}[2]{#2}
\providecommand{\eprint}[2]{\texttt{\href{#1}{#2}}}
\newcommand*{\amp}{\&}
% \newcommand*{\citein}[2][]{\textnormal{\textcite[#1]{#2}}%\addtocategory{extras}{#2}
% }
\newcommand*{\citein}[2][]{\textnormal{\textcite[#1]{#2}}%\addtocategory{extras}{#2}
}
\newcommand*{\citebi}[2][]{\textcite[#1]{#2}%\addtocategory{extras}{#2}
}
\newcommand*{\subtitleproc}[1]{}
\newcommand*{\chapb}{ch.}

% \def\arxivp{}
% \def\mparcp{}
% \def\philscip{}
% \def\biorxivp{}
% \newcommand*{\arxivsi}{\texttt{arXiv} eprints available at \url{http://arxiv.org/}.\\}
% \newcommand*{\mparcsi}{\texttt{mp\_arc} eprints available at \url{http://www.ma.utexas.edu/mp_arc/}.\\}
% \newcommand*{\philscisi}{\texttt{philsci} eprints available at \url{http://philsci-archive.pitt.edu/}.\\}
% \newcommand*{\biorxivsi}{\texttt{bioRxiv} eprints available at \url{http://biorxiv.org/}.\\}
\newcommand*{\arxiveprint}[1]{%\global\def\arxivp{\arxivsi}%\citeauthor{0arxivcite}\addtocategory{ifarchcit}{0arxivcite}%eprint
\texttt{\urlalt{https://arxiv.org/abs/#1}{arXiv:\hspace{0pt}#1}}%
%\texttt{\href{http://arxiv.org/abs/#1}{\protect\url{arXiv:#1}}}%
%\renewcommand{\arxivnote}{\texttt{arXiv} eprints available at \url{http://arxiv.org/}.}
}
\newcommand*{\mparceprint}[1]{%\global\def\mparcp{\mparcsi}%\citeauthor{0mparccite}\addtocategory{ifarchcit}{0mparccite}%eprint
\texttt{\urlalt{http://www.ma.utexas.edu/mp_arc-bin/mpa?yn=#1}{mp\_arc:\hspace{0pt}#1}}%
%\texttt{\href{http://www.ma.utexas.edu/mp_arc-bin/mpa?yn=#1}{\protect\url{mp_arc:#1}}}%
%\providecommand{\mparcnote}{\texttt{mp_arc} eprints available at \url{http://www.ma.utexas.edu/mp_arc/}.}
}
\newcommand*{\philscieprint}[1]{%\global\def\philscip{\philscisi}%\citeauthor{0philscicite}\addtocategory{ifarchcit}{0philscicite}%eprint
\texttt{\urlalt{http://philsci-archive.pitt.edu/archive/#1}{PhilSci:\hspace{0pt}#1}}%
%\texttt{\href{http://philsci-archive.pitt.edu/archive/#1}{\protect\url{PhilSci:#1}}}%
%\providecommand{\mparcnote}{\texttt{philsci} eprints available at \url{http://philsci-archive.pitt.edu/}.}
}
\newcommand*{\biorxiveprint}[1]{%\global\def\biorxivp{\biorxivsi}%\citeauthor{0arxivcite}\addtocategory{ifarchcit}{0arxivcite}%eprint
\texttt{\urlalt{https://doi.org/10.1101/#1}{bioRxiv doi:\hspace{0pt}10.1101/#1}}%
%\texttt{\href{http://arxiv.org/abs/#1}{\protect\url{arXiv:#1}}}%
%\renewcommand{\arxivnote}{\texttt{arXiv} eprints available at \url{http://arxiv.org/}.}
}
\newcommand*{\osfeprint}[1]{%
\texttt{\urlalt{https://doi.org/10.17605/osf.io/#1}{Open Science Framework doi:10.17605/osf.io/#1}}%
}

\usepackage{graphicx}
%\usepackage{wrapfig}
%\usepackage{tikz-cd}

\PassOptionsToPackage{hyphens}{url}\usepackage[hypertexnames=false]{hyperref}
\usepackage[depth=4]{bookmark}
\hypersetup{colorlinks=true,bookmarksnumbered,pdfborder={0 0 0.25},citebordercolor={0.2667 0.4667 0.6667},citecolor=mypurpleblue,linkbordercolor={0.6667 0.2 0.4667},linkcolor=myredpurple,urlbordercolor={0.1333 0.5333 0.2},urlcolor=mygreen,breaklinks=true,pdftitle={\pdftitle},pdfauthor={\pdfauthor}}
% \usepackage[vertfit=local]{breakurl}% only for arXiv
\providecommand*{\urlalt}{\href}

%%% Layout. I do not know on which kind of paper the reader will print the
%%% paper on (A4? letter? one-sided? double-sided?). So I choose A5, which
%%% provides a good layout for reading on screen and save paper if printed
%%% two pages per sheet. Average length line is 66 characters and page
%%% numbers are centred.
\ifafour\setstocksize{297mm}{210mm}%{*}% A4
\else\setstocksize{210mm}{5.5in}%{*}% 210x139.7
\fi
\settrimmedsize{\stockheight}{\stockwidth}{*}
\setlxvchars[\normalfont] %313.3632pt for a 66-characters line
\setxlvchars[\normalfont]
\setlength{\trimtop}{0pt}
\setlength{\trimedge}{\stockwidth}
\addtolength{\trimedge}{-\paperwidth}
% The length of the normalsize alphabet is 133.05988pt - 10 pt = 26.1408pc
% The length of the normalsize alphabet is 159.6719pt - 12pt = 30.3586pc
% Bringhurst gives 32pc as boundary optimal with 69 ch per line
% The length of the normalsize alphabet is 191.60612pt - 14pt = 35.8634pc
\ifafour\settypeblocksize{*}{32pc}{1.618} % A4
%\setulmargins{*}{*}{1.667}%gives 5/3 margins % 2 or 1.667
\else\settypeblocksize{*}{26pc}{1.618}% nearer to a 66-line newpx and preserves GR
\fi
\setulmargins{*}{*}{1}%gives equal margins
\setlrmargins{*}{*}{*}
\setheadfoot{\onelineskip}{2.5\onelineskip}
\setheaderspaces{*}{2\onelineskip}{*}
\setmarginnotes{2ex}{10mm}{0pt}
\checkandfixthelayout[nearest]
\fixpdflayout
%%% End layout
%% this fixes missing white spaces
\pdfmapline{+dummy-space <dummy-space.pfb}\pdfinterwordspaceon%

%%% Sectioning
\newcommand*{\asudedication}[1]{%
{\par\centering\textit{#1}\par}}
\newenvironment{acknowledgements}{\section*{Thanks}\addcontentsline{toc}{section}{Thanks}}{\par}
\makeatletter\renewcommand{\appendix}{\par
  \bigskip{\centering
   \interlinepenalty \@M
   \normalfont
   \printchaptertitle{\sffamily\appendixpagename}\par}
  \setcounter{section}{0}%
  \gdef\@chapapp{\appendixname}%
  \gdef\thesection{\@Alph\c@section}%
  \anappendixtrue}\makeatother
\counterwithout{section}{chapter}
\setsecnumformat{\upshape\csname the#1\endcsname\quad}
\setsecheadstyle{\large\bfseries\sffamily%
\raggedright}
\setsubsecheadstyle{\bfseries\sffamily%
\raggedright}
%\setbeforesecskip{-1.5ex plus 1ex minus .2ex}% plus 1ex minus .2ex}
%\setaftersecskip{1.3ex plus .2ex }% plus 1ex minus .2ex}
%\setsubsubsecheadstyle{\bfseries\sffamily\slshape\raggedright}
%\setbeforesubsecskip{1.25ex plus 1ex minus .2ex }% plus 1ex minus .2ex}
%\setaftersubsecskip{-1em}%{-0.5ex plus .2ex}% plus 1ex minus .2ex}
\setsubsecindent{0pt}%0ex plus 1ex minus .2ex}
\setparaheadstyle{\bfseries\sffamily%
\raggedright}
\setcounter{secnumdepth}{2}
\setlength{\headwidth}{\textwidth}
\newcommand{\addchap}[1]{\chapter*[#1]{#1}\addcontentsline{toc}{chapter}{#1}}
\newcommand{\addsec}[1]{\section*{#1}\addcontentsline{toc}{section}{#1}}
\newcommand{\addsubsec}[1]{\subsection*{#1}\addcontentsline{toc}{subsection}{#1}}
\newcommand{\addpara}[1]{\paragraph*{#1.}\addcontentsline{toc}{subsubsection}{#1}}
\newcommand{\addparap}[1]{\paragraph*{#1}\addcontentsline{toc}{subsubsection}{#1}}

% Headers and footers
\copypagestyle{manaart}{plain}
\makeheadrule{manaart}{\headwidth}{0.5\normalrulethickness}
\makeoddhead{manaart}{%
{\footnotesize%\sffamily%
\scshape\headauthor}}{}{{\footnotesize\sffamily%
\headtitle}}
\makeoddfoot{manaart}{}{\thepage}{}
%\newcommand*\autanet{\includegraphics[height=\heightof{M}]{autanet.pdf}}
\definecolor{mygray}{gray}{0.333}
\iftypodisclaim%
\ifafour\newcommand\addprintnote{\begin{picture}(0,0)%
\put(245,149){\makebox(0,0){\rotatebox{90}{\tiny\color{mygray}\textsf{This
            document is designed for screen reading and
            two-up printing on A4 or Letter paper}}}}%
\end{picture}}% A4
\else\newcommand\addprintnote{\begin{picture}(0,0)%
\put(176,112){\makebox(0,0){\rotatebox{90}{\tiny\color{mygray}\textsf{This
            document is designed for screen reading and
            two-up printing on A4 or Letter paper}}}}%
\end{picture}}\fi%afourtrue
\makeoddfoot{plain}{}{\makebox[0pt]{\thepage}\addprintnote}{}
\else
\makeoddfoot{plain}{}{\makebox[0pt]{\thepage}}{}
\fi%typodisclaimtrue
\makeoddhead{plain}{}{}{\footnotesize\reporthead}

% \copypagestyle{manainitial}{plain}
% \makeheadrule{manainitial}{\headwidth}{0.5\normalrulethickness}
% \makeoddhead{manainitial}{%
% \footnotesize\sffamily%
% \scshape\headauthor}{}{\footnotesize\sffamily%
% \headtitle}
% \makeoddfoot{manaart}{}{\thepage}{}

\pagestyle{manaart}

\setlength{\droptitle}{-3.9\onelineskip}
\pretitle{\begin{center}\LARGE\sffamily%
\bfseries}
\posttitle{\bigskip\end{center}}

\makeatletter\newcommand*{\atf}{\includegraphics[%trim=1pt 1pt 0pt 0pt,
totalheight=\heightof{@}]{atblack.png}}\makeatother
\providecommand{\affiliation}[1]{\textsl{\textsf{\footnotesize #1}}}
\providecommand{\epost}[1]{\texttt{\footnotesize\textless#1\textgreater}}
\providecommand{\email}[2]{\href{mailto:#1ZZ@#2 ((remove ZZ))}{#1\protect\atf#2}}

\preauthor{\vspace{-0.5\baselineskip}\begin{center}
\normalsize\sffamily%
\lineskip  0.5em}
\postauthor{\par\end{center}}
\predate{\DTMsetdatestyle{mydate}\begin{center}\footnotesize}
\postdate{\end{center}\vspace{-\medskipamount}}
\usepackage[british]{datetime2}
\DTMnewdatestyle{mydate}%
{% definitions
\renewcommand*{\DTMdisplaydate}[4]{%
\number##3\ \DTMenglishmonthname{##2} ##1}%
\renewcommand*{\DTMDisplaydate}{\DTMdisplaydate}%
}
\DTMsetdatestyle{mydate}


\setfloatadjustment{figure}{\footnotesize}
\captiondelim{\quad}
\captionnamefont{\footnotesize\sffamily%
}
\captiontitlefont{\footnotesize}
\firmlists*
\midsloppy

% handling orphan/widow lines, memman.pdf
% \clubpenalty=10000
% \widowpenalty=10000
% \raggedbottom
% Downes, memman.pdf
\clubpenalty=9996
\widowpenalty=9999
\brokenpenalty=4991
\predisplaypenalty=10000
\postdisplaypenalty=1549
\displaywidowpenalty=1602

\selectlanguage{british}\frenchspacing
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Paper's details %%%%
\title{\propertitle%\\
%  {\large A geometric commentary on maximum-entropy proofs}% ***
}
\author{%
\hspace*{\stretch{1}}%
\parbox{0.5\linewidth}%\makebox[0pt][c]%
{\protect\centering C\"uneyt\\%
\footnotesize\epost{\email{cuneyt.guzey}{ntnu.no}}}%
\hspace*{\stretch{1}}%
\parbox{0.5\linewidth}%\makebox[0pt][c]%
{\protect\centering Daniela\\%
\footnotesize\epost{\email{daniela.bragantini}{ntnu.no}}}%
\hspace*{\stretch{1}}%
\\[\jot]\hspace*{\stretch{1}}%
\parbox{0.5\linewidth}%\makebox[0pt][c]%
{\protect\centering Luca\\%
\footnotesize\epost{\email{piero.mana}{ntnu.no}}}%
\hspace*{\stretch{1}}%
\parbox{0.5\linewidth}%\makebox[0pt][c]%
{\protect\centering Yasser\\%
\footnotesize\epost{\email{yasser.roudi}{ntnu.no}}}%
\hspace*{\stretch{1}}%
%\quad\href{https://orcid.org/0000-0002-6070-0784}{\protect\includegraphics[scale=0.16]{orcid_32x32.png}\textsc{orcid}:0000-0002-6070-0784}%
}

\date{Draft of \today\ (first drafted \firstdraft)}
%\date{\firstpublished; updated \updated}

%@@@@@@@@@@ new macros @@@@@@@@@@
% Common ones - uncomment as needed
%\providecommand{\nequiv}{\not\equiv}
%\providecommand{\coloneqq}{\mathrel{\mathop:}=}
%\providecommand{\eqqcolon}{=\mathrel{\mathop:}}
%\providecommand{\varprod}{\prod}
\newcommand*{\de}{\partialup}%partial diff
\newcommand*{\pu}{\piup}%constant pi
\newcommand*{\delt}{\deltaup}%Kronecker, Dirac
%\newcommand*{\eps}{\varepsilonup}%Levi-Civita, Heaviside
%\newcommand*{\riem}{\zetaup}%Riemann zeta
%\providecommand{\degree}{\textdegree}% degree
%\newcommand*{\celsius}{\textcelsius}% degree Celsius
%\newcommand*{\micro}{\textmu}% degree Celsius
%\newcommand*{\I}{\mathrm{i}}%imaginary unit
%\newcommand*{\e}{\mathrm{e}}%Neper
\newcommand*{\di}{\mathrm{d}}%differential
%\newcommand*{\Di}{\mathrm{D}}%capital differential
%\newcommand*{\planckc}{\hslash}
%\newcommand*{\avogn}{N_{\textrm{A}}}
%\newcommand*{\NN}{\bm{\mathrm{N}}}
%\newcommand*{\ZZ}{\bm{\mathrm{Z}}}
%\newcommand*{\QQ}{\bm{\mathrm{Q}}}
\newcommand*{\RR}{\bm{\mathrm{R}}}
\newcommand*{\CC}{\bm{\mathrm{C}}}
%\newcommand*{\nabl}{\bm{\nabla}}%nabla
%\DeclareMathOperator{\lb}{lb}%base 2 log
%\DeclareMathOperator{\tr}{tr}%trace
%\DeclareMathOperator{\card}{card}%cardinality
%\DeclareMathOperator{\im}{Im}%im part
%\DeclareMathOperator{\re}{Re}%re part
%\DeclareMathOperator{\sgn}{sgn}%signum
%\DeclareMathOperator{\ent}{ent}%integer less or equal to
%\DeclareMathOperator{\Ord}{O}%same order as
%\DeclareMathOperator{\ord}{o}%lower order than
%\newcommand*{\incr}{\triangle}%finite increment
\newcommand*{\defd}{\coloneqq}
\newcommand*{\defs}{\eqqcolon}
%\newcommand*{\Land}{\bigwedge}
%\newcommand*{\Lor}{\bigvee}
%\newcommand*{\lland}{\mathbin{\ \land\ }}
%\newcommand*{\llor}{\mathbin{\ \lor\ }}
%\newcommand*{\lonlyif}{\mathbin{\Rightarrow}}%implies
%\newcommand*{\limplies}{\mathbin{\Rightarrow}}%implies
\newcommand*{\mimplies}{\Rightarrow}%implies
%\newcommand*{\liff}{\mathbin{\Leftrightarrow}}%if and only if
%\newcommand*{\cond}{\mathpunct{|}}%conditional sign (in probabilities)
%\newcommand*{\lcond}{\mathpunct{|\ }}%conditional sign (in probabilities)
%\newcommand*{\bigcond}{\mathpunct{\big|}}%conditional sign (in probabilities)
%\newcommand*{\lbigcond}{\mathpunct{\big|\ }}%conditional sign (in probabilities)
\newcommand*{\suchthat}{\mid}%{\mathpunct{|}}%such that (eg in sets)
%\newcommand*{\bigst}{\mathpunct{\big|}}%such that (eg in sets)
%\newcommand*{\with}{\colon}%with (list of indices)
%\newcommand*{\mul}{\times}%multiplication
%\newcommand*{\inn}{\cdot}%inner product
%\newcommand*{\dotv}{\mathord{\,\cdot\,}}%variable place
%\newcommand*{\comp}{\circ}%composition of functions
%\newcommand*{\con}{\mathbin{:}}%scal prod of tensors
%\newcommand*{\equi}{\sim}%equivalent to 
\renewcommand*{\asymp}{\simeq}%equivalent to 
%\newcommand*{\corr}{\mathrel{\hat{=}}}%corresponds to
%\providecommand{\varparallel}{\ensuremath{\mathbin{/\mkern-7mu/}}}%parallel (tentative symbol)
\renewcommand{\le}{\leqslant}%less or equal
\renewcommand{\ge}{\geqslant}%greater or equal
%\DeclarePairedDelimiter\clcl{[}{]}
\DeclarePairedDelimiter\clop{[}{[}
%\DeclarePairedDelimiter\opcl{]}{]}
%\DeclarePairedDelimiter\opop{]}{[}
%\DeclarePairedDelimiter\abs{\lvert}{\rvert}
%\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\set{\{}{\}}
%\DeclareMathOperator{\pr}{P}%probability
\newcommand*{\pf}{\mathrm{p}}%probability
\newcommand*{\p}{\mathrm{P}}%probability
%\newcommand*{\tf}{\mathrm{T}}%probability
\renewcommand*{\|}{\mathpunct{|}}
%\newcommand*{\lcond}{\mathpunct{|\ }}%conditional sign (in probabilities)
\newcommand*{\bigcond}{\mathpunct{\big|\ }}%conditional sign (in probabilities)
%\newcommand*{\lbigcond}{\mathpunct{\big|\ }}%conditional sign (in probabilities)
%\newcommand*{\+}{\lor}
%\renewcommand{\*}{\land}
\newcommand*{\sect}{\S}% Sect.~
\newcommand*{\sects}{\S\S}% Sect.~
\newcommand*{\chap}{ch.}%
\newcommand*{\chaps}{chs}%
\newcommand*{\bref}{ref.}%
\newcommand*{\brefs}{refs}%
%\newcommand*{\fn}{fn}%
\newcommand*{\eqn}{eq.}%
\newcommand*{\eqns}{eqs}%
\newcommand*{\fig}{fig.}%
\newcommand*{\figs}{figs}%
\newcommand*{\vs}{{vs}}
%\newcommand*{\etc}{{etc.}}
%\newcommand*{\ie}{{i.e.}}
%\newcommand*{\ca}{{c.}}
%\newcommand*{\eg}{{e.g.}}
\newcommand*{\foll}{{ff.}}
%\newcommand*{\viz}{{viz}}
\newcommand*{\cf}{{cf.}}
%\newcommand*{\Cf}{{Cf.}}
%\newcommand*{\vd}{{v.}}
\newcommand*{\etal}{{et al.}}
%\newcommand*{\etsim}{{et sim.}}
%\newcommand*{\ibid}{{ibid.}}
%\newcommand*{\sic}{{sic}}
%\newcommand*{\id}{\mathte{I}}%id matrix
%\newcommand*{\nbd}{\nobreakdash}%
%\newcommand*{\bd}{\hspace{0pt}}%
%\def\hy{-\penalty0\hskip0pt\relax}
%\newcommand*{\labelbis}[1]{\tag*{(\ref{#1})$_\text{r}$}}
%\newcommand*{\mathbox}[2][.8]{\parbox[t]{#1\columnwidth}{#2}}
%\newcommand*{\zerob}[1]{\makebox[0pt][l]{#1}}
\newcommand*{\tprod}{\mathop{\textstyle\prod}\nolimits}
\newcommand*{\tsum}{\mathop{\textstyle\sum}\nolimits}
%\newcommand*{\tint}{\begingroup\textstyle\int\endgroup\nolimits}
%\newcommand*{\tland}{\mathop{\textstyle\bigwedge}\nolimits}
%\newcommand*{\tlor}{\mathop{\textstyle\bigvee}\nolimits}
%\newcommand*{\sprod}{\mathop{\textstyle\prod}}
%\newcommand*{\ssum}{\mathop{\textstyle\sum}}
%\newcommand*{\sint}{\begingroup\textstyle\int\endgroup}
%\newcommand*{\sland}{\mathop{\textstyle\bigwedge}}
%\newcommand*{\slor}{\mathop{\textstyle\bigvee}}
\newcommand*{\T}{^\intercal}%transpose
\newcommand*{\E}{\mathrm{E}}
\DeclarePairedDelimiter\expp{(}{)}
\newcommand*{\expe}{\E\expp}%round
%\newcommand*{\expeb}{\E\clcl}%square
%%\newcommand*{\QEM}%{\textnormal{$\Box$}}%{\ding{167}}
%\newcommand*{\qem}{\leavevmode\unskip\penalty9999 \hbox{}\nobreak\hfill
%\quad\hbox{\QEM}}

\definecolor{notecolour}{RGB}{68,170,153}
%\newcommand*{\puzzle}{{\fontencoding{U}\fontfamily{fontawesometwo}\selectfont\symbol{225}}}
\newcommand*{\puzzle}{\maltese}
\newcommand{\mynote}[1]{ {\color{notecolour}\puzzle\ #1}}
\newcommand*{\widebar}[1]{{\mkern1.5mu\skew{2}\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}}

%\DeclareMathOperator*{\argsup}{arg\,sup}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\diag}{diag}
\newcommand*{\ptext}[1]{\text{\small #1}}
\newcommand*{\dob}{degree of belief}
\newcommand*{\dobs}{degrees of belief}
\newcommand*{\snp}{\textsc{snp}}
\newcommand*{\yD}{D}
\newcommand*{\yG}{G}
\newcommand*{\yS}{S}
\newcommand*{\yI}{I}
\newcommand*{\sH}{H}
\newcommand*{\mI}{I}
\newcommand*{\yGv}{G}
\newcommand*{\yGi}[1]{G^{(#1)}}
\newcommand*{\ySv}{S}
\newcommand*{\ySi}[1]{S^{(#1)}}
\newcommand*{\yx}{\bm{x}}
\newcommand*{\yy}{\bm{y}}
\newcommand*{\ysum}{\tsum}
\newcommand*{\yprod}{\tprod}
\newcommand*{\ys}{\sigma}
\newcommand*{\yg}{\gamma}
\newcommand*{\gn}{l}
\newcommand*{\ysi}[1]{\ys^{(#1)}}
\newcommand*{\ygi}[1]{\yg^{(#1)}}
\newcommand*{\yso}{\ysi{0}}
\newcommand*{\ygo}{\ygi{0}}
\newcommand*{\yFs}{\bm{S}}
\newcommand*{\yfs}{\bm{s}}
\newcommand*{\yFg}{\bm{G}}
\newcommand*{\yfg}{\bm{g}}
\newcommand*{\yng}{v}
\newcommand*{\yv}{\xi}
\newcommand*{\yvi}[1]{\yv^{(#1)}}
\newcommand*{\yF}{\bm{X}}
\newcommand*{\yf}{\bm{x}}
\newcommand*{\ye}{\bm{e}}
\newcommand*{\yCs}{C_{\sigma}}
\newcommand*{\yCg}{C_{\gamma}}
\newcommand*{\yIc}{I_{\gn}}
\newcommand*{\yCgd}{\yCg^{+}}
\newcommand*{\yCgn}{\yCg^{-}}
\newcommand*{\yCd}{C^{+}}
\newcommand*{\yCn}{C^{-}}
\newcommand*{\yIo}{I_{0}}
\newcommand*{\yA}{\varTheta}
\newcommand*{\ya}{\bm{\theta}}
%@@@@@@@@@@ new macros end @@@@@@@@@@

\firmlists
\begin{document}
\captiondelim{\quad}\captionnamefont{\footnotesize}\captiontitlefont{\footnotesize}
\selectlanguage{british}\frenchspacing

%%% Title and abstract %%%
\maketitle
\ifpublic
\abstractrunin
\abslabeldelim{}
\renewcommand*{\abstractname}{}
\setlength{\absleftindent}{0pt}
\setlength{\absrightindent}{0pt}
\setlength{\abstitleskip}{-\absparindent}
\begin{abstract}\labelsep 0pt%
  \noindent Research notes
% \par%\\[\jot]
% \noindent
% {\footnotesize PACS: ***}\qquad%
% {\footnotesize MSC: ***}%
%\qquad{\footnotesize Keywords: ***}
\end{abstract}\fi

\selectlanguage{british}\frenchspacing
% \asudedication{\small ***}
% \vspace{\bigskipamount}

% \setlength{\epigraphwidth}{.7\columnwidth}
% %\epigraphposition{flushright}
% \epigraphtextposition{flushright}
% %\epigraphsourceposition{flushright}
% \epigraphfontsize{\footnotesize}
% \setlength{\epigraphrule}{0pt}
% %\setlength{\beforeepigraphskip}{0pt}
% %\setlength{\afterepigraphskip}{0pt}
% \epigraph{\emph{text}}{source}

\iffalse\noindent\emph{\footnotesize Note: Dear Peer, this manuscript is
  peer-reviewed by \emph{you}. I'm grateful if you let me know of any
  faults in its premisses, logic, evidence, and of any other criticisms you
  may have.}\fi

\mynote{2018--10--21: This is the old version, before the approach via conditional \dobs}

\section{Introductory notes}
\label{sec:pre_intro}


\subsection{Preliminary remarks about Bayesian  theory}
\label{sec:pre_prelim_remarks}

Bayesian  theory is not just a set of new, better recipes meant
to replace old ones. It also requires a different -- and simpler -- mindset
about problems of inference. Three points are especially important:

1. The only purpose of Bayesian theory is to give the \dob\ in some
statements -- more exactly, \enquote{propositions}
\citep{copietal1953_r2014,barwiseetal1999_r2003} -- given other statements
that may concern data, facts, hypotheses. For example, Bayesian theory can
tell us that we have a \dob\ $x$ in hypothesis $A$, given some data $C$ and
initial information $I$, and a \dob\ $y$ in hypothesis $B$ given the same
conditions:
\begin{equation*}
  \p(A \| C, I ) = x, \qquad   \p(B \| C, I ) = y.
\end{equation*}
That's all there is to it. We can then use these \dobs\ as we like; in
particular, we can use them within decision theory to choose courses of
action \citep{raiffaetal1961_r2000,prattetal1995_r1996,soxetal1988_r2013}.
But notions like \enquote{statistical significance}, \enquote{acceptance
  level}, \enquote{confidence}, and similar are foreign to Bayesian theory;
or at best they're just secondary notions.

\medskip

2. Bayesian theory is an extension of formal logic, the truth calculus. In
fact we'll call it \emph{plausibility calculus} from now on.

In formal logic, to prove a theorem we need some axioms to start from.
These may partly include experimental facts or data, but they always also
include assumptions that are purely conjectural. It's impossible to avoid
this conjectural element \citep[see for example][]{harding1976}.
\footnote{This impossibility is well known in modern science; we can quote
  Poincar\'e \citey{poincare1902_r1992}: \enquote{But upon more reflection
    we realize the position held by hypothesis; we see that the
    mathematician wouldn't know how to do without it, and the experimenter
    can't do without it at all} \parentext{Introduction}; \enquote{Every
    generalization is a hypothesis} \parentext{\chap~IX, p.~176}. Duhem
  \citey{duhem1906_t1991}: \enquote{In sum, the physicist can never subject
    an isolated hypothesis to experimental test, but only a whole group of
    hypotheses; when the experiment is in disagreement with his
    predictions, what he learns is that at least one of the hypotheses
    constituting this group is unacceptable and ought to be modified; but
    the experiment does not designate which one should be changed}
  \parentext{\sect~VI.2, p.~187}; \enquote{Unlike the reduction to
    absurdity employed by geometers, experimental contradiction does not
    have the power to transform a physical hypothesis into an indisputable
    truth; in order to confer this power on it, it would be necessary to
    enumerate completely the various hypotheses which may cover a
    determinate group of phenomena; but the physicist is never sure he has
    exhausted all the imaginable assumptions} \parentext{\sect~VI.3,
    p.~190}; \enquote{the realization and interpretation of no matter what
    experiment in physics imply adherence to a whole set of theoretical
    propositions} \parentext{\sect~VI.5, p.~200}. Medawar
  \citey{medawar1963}: \enquote{the starting point of induction, naive
    observation, innocent observation, is a mere philosophic fiction. There
    is no such thing as unprejudiced observation} Jeffreys [quote][ref].}
Likewise, in the plausibility calculus we need to specify initial \dobs.
These may originate in data, but they always also include additional
assumptions. The motto \enquote{let the data speak for themselves} is
simply impossible.

The difference between Bayesian methods and traditional methods is
\emph{not} that the former need additional assumptions while the latter
don't. Rather, Bayesian methods make these assumptions explicit, while
traditional methods hide them. This is the reason why many traditional
results can be obtained as special cases of Bayesian ones.

\medskip

3. Conditional \dobs\ like $P(A \| B)$ do not express a causal connection
between $A$ and $B$, but an \emph{informational} connection. In that
conditional \dob, $A$ could be the cause of $B$, or $B$ of $A$, or neither
could be the cause of the other. The classical example of this is
\begin{equation}
  \label{eq:pre_rain_clouds}
  \p(\text{\small clouds in the sky} \| \text{\small rain on the pavement}, I) > 0.5,
\end{equation}
not because the rain is the cause of the clouds, but because its presence gives us
\emph{relevant information} about the cloudiness of the sky.

\medskip

The previous remarks may appear pedantic, but they're important lest we
misuse Bayesian methods.

\subsection{What is the question?}
\label{sec:pre_what_question}

\mynote{Luca: the following thoughts may be naive; I must still read
  \citep{stingoetal2015} and \citep{bushetal2012}}

We want to assess the informational relevance between some genetic
variations $\set{\yG}$ and (combinations of) insomnia symptoms $\set{\yS}$
in the Norwegian or European population. To assess this relevance we use
data $\yD$ from a population sample. Some assumptions or background
information $\yI$ are also always present in our assessment.

The traditional approach to this kind of problems is to set two hypotheses
against each other: \enquote{there is a correlation} vs \enquote{there
  isn't a correlation}, and to assess which is more \enquote{significant}
in view of the data. Mathematically this corresponds to a dichotomy between
an exactly zero value and non-zero values of a correlation-like quantity.

Here we approach this problem differently. Rather than contrasting zero
against non-zero values, we simply calculate how \emph{relevant} one
quantity is to make inferences about the other quantity.\mynote{check
  \citep{stephensetal2009}: they seem to have a reference with a similar
  philosophy} One quantity, for example $\yG$, is \emph{inferentially} or
\emph{informationally} relevant when the knowledge of its value leads us to
have a different degree of belief regarding the value of the other, for
example $\yS$, compared to when we don't know its value. In other words,
the \dobs\ $\p(\yS \| \yG \,\yD \yI)$ and $\p(\yS \| \yD \yI)$ are
numerically different. If these two \dobs\ are approximately equal
then the particular genetic variation $\yG$ are \emph{ir}relevant for our
prediction of the insomnia symptom $\yS$. The same conclusion holds with
$\yG$ and $\yS$ exchanged: the plausibility calculus says that
\begin{equation}
  \label{eq:pre_exchange_irrelevance}
  \p(\yS \| \yG \,\yD \yI) = \p(\yS \| \yD \yI)
  \quad\Longleftrightarrow\quad
    \p(\yG \| \yS \,\yD \yI) = \p(\yG \| \yD \yI)
\end{equation}
if $\p(\yS \| \yD \yI)$, $\p(\yG \| \yD \yI)$ aren't zero.

This measure of relevance can be extended to sets of (combinations of)
symptoms $\set{\yS}$ and of genetic variations $\set{\yG}$ by using the
conditional entropy
\citep{shannon1948,kelly1956}[\sect~14.7]{pressetal1988_r2007}[\chap~2]{coveretal1991_r2006}
\begin{equation}
  \label{eq:pre_conditional_entropy}
  \sH(\set{\yS} \| \set{\yG}, \yD\yI) \defd
  -\sum_{\yG} \p(\yG \| \yD\yI)\,
  \sum_{\yS} \p(\yS \|\yG\,\yD\yI) \ln\p(\yS \|\yG\,\yD\yI),
\end{equation}
which is zero only if $\yG$ gives us certainty about $\yS$, and is equal to
the entropy 
\begin{equation}
  \sH(\set{\yS} \| \yD\yI) \defd
-\sum_{\yS}\p(\yS \|\yD\yI) \ln\p(\yS \|\yD\yI)
  \label{eq:pre_entropy_G}
\end{equation}
if $\yG$ is irrelevant for predicting $\yS$
\citep[\sect~14.7]{pressetal1988_r2007}[\chap~2]{coveretal1991_r2006}.
Another, symmetric measure of relevance is the mutual information,
discussed in \sect~\ref{sec:pre_joint_prob}.

If we find that there is mutual informational relevance between genetic
variations and insomnia symptoms, we can conclude from biologic reasons
that those variations must have a direct or indirect influence on the
symptoms, for example they may give susceptibility to insomnia.

There are three main ways to calculate the conditional \dobs: By
calculating first $\p(\yS\yG \|\yD\yI)$, or $\p(\yS \|\yG\,\yD\yI)$, or
$\p(\yG \|\yS\,\yD\yI)$. Also, we can consider all possible combinations of
genetic variations from the outset, or consider combination of few
variations, gradually increasing the numbers. We shall try all these
approaches and see whether their results are mutually consistent.


\subsection{Why exchangeability}
\label{sec:pre_why_exchangeability}



The sentence \enquote{the \dob\ in an insomnia symptom given a genetic
  variation} is vague. What we mean is our guess that a given individual
with that variation presents that symptom. Our guess about a particular
individuals in the full population is updated from our knowledge about
individuals we have sampled. Our guess about a new individual is affected
by the sample data only insofar we believe those data to be representative
for that individual.

The notion of exchangeability expresses this representativeness in
 terms of \dobs, as explained at length by de~Finetti
\citey{definetti1931,definetti1937,definetti1938}. Denote by $\ySi{i}_s$
the statement that individual $i$ has symptom $\yS_s$, and likewise with
$\yGi{i}_g$ for the combination of genetic variations $g$. The fact that we
believe, for inferential purposes, that the individuals from a population
having the same genetic variation $g$ are representative of one another is
expressed by
\begin{multline}
\label{def_exchangeability}
  \p(\ySi{1}_{s_1}\, \ySi{2}_{s_2}\, \ySi{3}_{s_3}\,\dotso \|
  \yGi{1}_{g}\, \yGi{2}_{g}\, \yGi{3}_{g}\,\dotso
  \;\yI)
  ={}\\
  \p(\ySi{1}_{s_{\pi(1)}}\, \ySi{2}_{s_{\pi(2)}}\, \ySi{3}_{s_{\pi(3)}}\,\dotso \|
  \yGi{1}_{g}\, \yGi{2}_{g}\, \yGi{3}_{g}\,\dotso
  \;\yI)
\end{multline}
where $\pi$ is an arbitrary permutation of the individuals' labels $i$. For
example,
\begin{multline}
\label{example_exchangeability}
  \p(\ySi{1}_{\text{c}}\, \ySi{2}_{\text{a}}\, \ySi{3}_{\text{b}}\,\dotso \|
  \yGi{1}_{g}\, \yGi{2}_{g}\, \yGi{3}_{g}\,\dotso
  \;\yI)
  ={}\\
  \p(\ySi{1}_{\text{b}}\, \ySi{2}_{\text{c}}\, \ySi{3}_{\text{a}}\,\dotso \|
  \yGi{1}_{g}\, \yGi{2}_{g}\, \yGi{3}_{g}\,\dotso
  \;\yI).
\end{multline}
This mathematical property is called \emph{exchangeability}.
If the number of individuals is finite it is called \emph{finite}
exchangeability; letting this number increase indefinitely we reach
\emph{infinite} exchangeability as a limit \citep{heathetal1976}.

Assuming exchangeability for each group of individuals sharing the same
genetic variation, the mathematical relations above generalize as follows.
Our \dob\ is the same if we exchange symptom labels among individuals
\emph{having the same genetic variation}; but it may be different if we
exchange symptom labels among individuals with different genetic
variations. The exact mathematical expression may look complicated; a
concrete example is
\begin{multline}
  \label{eq:pre_example_partial_exch}
  \p(\ySi{1}_{\text{a}}\, \ySi{2}_{\text{c}}\, \ySi{3}_{\text{d}}\,
  \ySi{4}_{\text{a}}\, \ySi{5}_{\text{b}}\, \ySi{6}_{\text{d}}\,\dotso \|
  \yGi{1}_{\alphaup}\, \yGi{2}_{\betaup}\, \yGi{3}_{\betaup}\,
  \yGi{4}_{\gammaup}\, \yGi{5}_{\alphaup}\, \yGi{6}_{\gammaup}\,\dotso
  \;\yI)
  ={}\\
  \p(\ySi{1}_{\text{b}}\, \ySi{2}_{\text{d}}\, \ySi{3}_{\text{c}}\,
  \ySi{4}_{\text{d}}\, \ySi{5}_{\text{a}}\, \ySi{6}_{\text{a}}\,\dotso \|
  \yGi{1}_{\alphaup}\, \yGi{2}_{\betaup}\, \yGi{3}_{\betaup}\,
  \yGi{4}_{\gammaup}\, \yGi{5}_{\alphaup}\, \yGi{6}_{\gammaup}\,\dotso
  \;\yI)
\end{multline}
where our \dob\ remains the same as we exchange symptoms $\text{a}$
and $\text{b}$ between individuals $1$ and $5$, both having genetic
variation $\alphaup$; symptoms $\text{c}$ and $\text{d}$ between
individuals $2$ and $3$, both having genetic variation $\betaup$; symptoms
$\text{a}$ and $\text{d}$ between individuals $4$ and $6$, both having
genetic variation $\gammaup$. These exchanges can involve an arbitrary
number of individuals, symptoms, genetic variations. This general property
is called \emph{partial} exchangeability
\cites{definetti1938,diaconisetal1980b,diaconis1988}[for a connection with
sampling theory see][]{sugden1982,sugden1993}.

Note that the property exemplified by \eqn~\eqref{eq:pre_example_partial_exch}
is more general than just separately stating exchangeability for the
distributions of \dobs\ in the individuals' sharing the same genetic
variations. Property~\eqref{eq:pre_example_partial_exch} allows data about
individuals with a genetic variation to be \emph{relevant} for prediction
of data about individuals with \emph{another} genetic variation, as we'll
see shortly.


\mynote{add: de Finetti's representation theorem for distributions of
  \dobs\ with the property above}

\subsection{Selection of variables and robustness}
\label{sec:pre_variable_selection_robustness}

Denote the presence of the genetic variation labelled $i$ by $\yGv_i$ and its
absence by $\lnot\yGv_i$. We can consider the  relevance of each
variation individually, say
\begin{equation}
  \label{eq:pre_example_relevance_onevariation}
  \p(\yS \|\yGv_1 \,\yD \yI),
\end{equation}
or of the combination of any number of variations, say
\begin{equation}
  \label{eq:pre_example_relevance_manyvariations}
  \p(\yS \|\yGv_1 \lnot\yGv_2 \lnot\yGv_3 \yGv_4 \,\yD \yI).
\end{equation}
The plausibility calculus allows us to assign all these \dobs\ for any
amount of data $\yD$ -- since they represent beliefs. If the number of
combinations is high compared with the number of data, however, our \dobs\
will usually change noticeably when updated with new data; we can say that
they are less \enquote{robust} to the acquisition of new data. This
robustness can be quantified in various ways to be discussed later.

From this point of view it makes sense to first consider each genetic
variation individually and then larger and larger combinations of
variations, as long as we see that our \dobs\ conditional on data
$\yD$ are robust.


\mynote{Jeffreys \citey{jeffreys1939_r1983} \sect~3.2 \emph{very} relevant
  to our problem! Also Broad \citey{broad1918}}

\mynote{See Jeffreys \citey[\sect~3.1, p.~124]{jeffreys1939_r1983} on the
  \dob\ to be given to the ratio values $\set{0,1}$: \enquote{In
    genetics the suggested values are usually intermediate, such as 1/2,
    1/4, and 3/8}. Also, \enquote{we cannot give a universal rule for them
    beyond the common-sense one, that if anybody does not know what his
    suggested value is, or whether there is one, he does not know what
    question he is asking and consequently does not know what his answer
    means. }}



\section{First approach: joint \dob\ and mutual information}
\label{sec:pre_joint_prob}

\subsection{Notation}\label{sec:pre_notation}
The following notation produces compact but readily understandable
formulae. Functions and operations on tuples $\yx \defd (x_1,\dotsc,x_C)$,
$\yy \defd (y_1,\dotsc,y_C)$, and numbers $a$ operate component-wise. For
example:
\begin{subequations}
    \label{eq:pre_example_notation}
  \begin{equation}
    \begin{gathered}
      \exp\yx \defd \bigl( \exp x_1, \dotsc, \exp x_C \bigr)
      \qquad
      \yx\yy \defd ( x_1\,y_1, \dotsc, x_C\,y_C )
      \\
      a\yx \defd (a x_1, \dotsc, a x_C)
      \qquad
      \yx^a \defd \bigl( {x_1}^a,\dotsc,{x_C}^a \bigr)
      \qquad\text{and so on.}
    \end{gathered}
  \end{equation}
The exception are the sum and multiplication operators $\ysum$, $\yprod$:
\begin{equation}
  \ysum\yx \defd x_1+\dotsb+x_C
  \qquad
  \yprod\yx \defd x_1  \dotsm x_C
\end{equation}
so that, for example, \[\ysum\ln(\yx/\yy) \defd \sum_{i=1}^C \ln(x_i/y_i).\]
Note also the conventions
  \begin{equation}
    \binom{a}{\yx} \defd \binom{a}{x_1, \dotsc, x_C} \defd
    \frac{a!}{x_1! \dotsm x_C!}
    \qquad
    \yprod\binom{\yy}{\yx} \defd \binom{y_1}{x_1}  \dotsm  \binom{y_C}{x_C},
  \end{equation}
\end{subequations}
where the first expression is the multinomial coefficient.



\subsection{Scheme of this approach}
\label{sec:pre_1st_approach_scheme}

Here is the way of thinking and general form of the calculations for this
approach. We'll see later if these calculations are practically feasible.

Denote by $N$ the size of the full population. The Norwegian or European
populations amount roughly to $5.3 \times 10^6$ and $740 \times 10^6$, but
we'll see later that it makes sense to consider the limit $N\to\infty$. Our
initial information $\yI$ says that each individual is characterized by two
groups of quantities or variates:
\begin{enumerate}
\item An insomnia variate $\ys \defd (\ys_1,\ys_2,\ys_3)\in \set{0,1}^3$
  with $\yCs\defd 8$ possible values. This variate consists of three
  binary variates representing the presence or absence of three insomnia
  symptoms. An individual with no insomnia symptoms (\enquote{control}) has
  therefore $\ys=(0,0,0)$. When convenient we shall use a binary-digit
  notation like $\ys=2\equiv (0,1,0)$ or $\ys=5\equiv(1,0,1)$.
\item A genetic variate
  $\yg \defd (\yg_1,\dotsc,\yg_{\gn})\in \set{0,1}^{\gn}$ with
  $\yCg \defd 2^{\gn}$ possible values. This variate consists of $\gn$
  binary variates, each representing the presence of either of two variants
  of a particular gene allele. We have data for $94$ allele pairs, but
  we'll often consider a smaller subset of pairs. When convenient we shall
  use a binary-digit notation also for this variate.
\end{enumerate}
The combined variate $\yv \defd (\ys,\yg)$ can thus assume
$C\defd \yCs\times\yCg$ %$ \approx 1.6 \times 10^{29}$
possible values, depending on how many gene alleles we consider.

We have data $\yD$ consisting of the values of these variates for a sample
of $n\defd 6029$ individuals. The values for individual $i$ are denoted
$\yvi{i} \defd \bigl( \ysi{i}, \ygi{i} \bigr)$, $i \in \set{1,\dotsc,n}$.

Denote the relative frequency of the insomnia-variate value $\ys$ in our
data by $s_{\ys}$, and the frequency distribution of all values by
$\yfs\defd(s_0,\dotsc,s_7)$. The frequency distribution is normalized,
$\ysum\yfs=1$. The relative frequency for the gene-variate value $\yg$ is
denoted $g_{\yg}$, and the frequency distribution
$\yfg \defd (g_0,\dotsc,g_{2^{\gn}})$. The relative frequency for the joint
variate value $(\ys,\yg)$ is $x_{\ys,\yg}$, and the frequency distribution
$\yf \defd (x_{0,0},\dotsc, x_{7,2^{\gn}})$. By marginalization we must
have $\sum_{\yg} x_{\ys,\yg} = s_{\ys}$ and
$\sum_{\ys} x_{\ys,\yg} = g_{\yg}$. We can also consider the relative
frequency of a particular gene allele, say the $j$th one; we'll denote it
by $g_{\yg_j}$.

\medskip

From our data $\yD$ and from some initial knowledge $\yI$ we want to make
inferences about two connected unknowns:
\begin{enumerate}[label=(\alph*)]
\item the frequency distributions of insomnia symptoms, of gene variations,
  and of both jointly in the full population of $N$ individuals. In other
  words, we must guess what these frequency distributions are, and
  therefore quantify our \dobs\ in their
  possible values. Let's denote these frequency distributions with the same
  symbols as for our data, but with capital letters: $\yFs$ is the
  frequency distribution of insomnia symptoms, $\yFg$ of gene variations,
  and $\yF$ the joint distribution of both. In formulae we want to assign
  values to
  \begin{equation}
    \label{eq:pre_goal_plausibilities}
    \pf(\yF \| \yD,\yI),\qquad
    \pf(\yFs \| \yD,\yI),\qquad
    \pf(\yFg \| \yD,\yI).
  \end{equation}
  With $N$ individuals, there are $\binom{N+C-1}{C-1}$ possible
  distributions $\yF$; also $\binom{N+\yCs-1}{\yCs-1}$ possible
  distributions $\yFs$  and $\binom{N+\yCg-1}{\yCg-1}$ possible
  distributions $\yFg$ \citep[\sect~2.1]{csiszaretal2004b}.
\item The symptoms and gene variations of an individual \enquote{$0$}
  chosen at random from the full population. That is, we want to quantify
  our degrees of belief in joint and separate possible values of these
  variates for this individual:
  \begin{equation}
    \label{eq:pre_goal_ind_plausibilities}
    \pf\bigl( \ysi{0}, \ygi{0} \| \yD, \yI \bigr),\qquad
    \pf\bigl( \ysi{0} \| \yD, \yI \bigr),\qquad
    \pf\bigl(  \ygi{0} \| \yD, \yI \bigr).
  \end{equation}
\end{enumerate}



% In the rest of this section we'll denote $\ysi{0}$, $\ygi{0}$ simply by
% $\yso$, $\ygo$ for brevity.


These two kinds of \dob\ are mathematically connected: If we knew the joint
frequency distribution $\yF$ in the full population, then our degree of
belief that individual $0$ have variates $\bigl( \yso,\ygo \bigr)$ would
be, by symmetry,
\begin{equation}
  \label{eq:pre_joint_prob_new_ind_F_known}
  \pf\bigl(\yso, \ygo \| \yF, \yI\bigr)  =
  \pf\bigl(\yso, \ygo \| \yF, \yD, \yI\bigr)  =
  X_{\yso, \ygo},
\end{equation}
that is, the frequency by which the value $\bigl(\yso, \ygo\bigr)$ appears
in the population. The conditionals in these \dobs\ indicate that we know
$\yF$ besides our initial knowledge $\yI$, and the first equality says that
the data $\yD$ would be irrelevant if we knew $\yF$. If $\yF$ is unknown,
then by the theorem of total \dob\ we have
\begin{multline}
  \label{eq:pre_joint_prob_new_ind_F_unknown}
  \pf\bigl(\yso, \ygo \| \yD, \yI\bigr)  =
  \sum_{\yF}  \pf\bigl(\yso, \ygo \| \yF, \yD, \yI\bigr) \;
  \pf( \yF \| \yD, \yI) \equiv{}\\
  \sum_{\yF}  X_{\yso, \ygo}\,  \pf( \yF \| \yD, \yI),
\end{multline}
where the sum comprises $\binom{N+C-1}{C-1}$ terms. This formula says that
our \dob\ in the individual's variates equals our expectation of the
frequency of those variates.
Formula~\eqref{eq:pre_joint_prob_new_ind_F_unknown} thus connects the degrees
of belief~\eqref{eq:pre_goal_plausibilities} and
\eqref{eq:pre_goal_ind_plausibilities}. Analogous equations hold for the
marginal frequency distributions $\yFs$, $\yFg$ and the variates $\yso$,
$\ygo$.


If the number of alleles and considered $\gn>\log_{2}n$, the $C$ possible
joint variate values are much more numerous than the data $n$; not all of
them can therefore appear in the data. Hence many of these gene variate
values have frequencies $g_{\yg}=0$ and consequently $x_{\ys,\yg}=0$.
Denote by $\yCgd$ the number of distinct gene variations present in the
data, and by $\yCgn$ the number of those absent. Let $\yCd$ and $\yCn$ have
the same meaning regarding the joint variate
$\bigl( \ysi{0}, \ygi{0} \bigr)$. Obviously $\yCg = \yCgd+\yCgn$,
$C=\yCd+\yCn$, and $\yCd \le \yCgd$.

As explained in \sect~\ref{sec:pre_what_question}, the relevance of our
knowledge of an individual's genetic data for our \dob\ about his or her
insomnia symptoms resides in the difference between the distributions
$\pf\bigl(\yso\| \ygo, \yD, \yI\bigr)$ and
$\pf\bigl(\yso \| \yD, \yI\bigr)$. This can be quantified as the difference
in the corresponding entropy $\sH\bigl(\yso \| \yD,\yI\bigr)$
and conditional entropy
$\sH\bigl(\yso \| \ygo; \yD,\yI)$, which is the
mutual information \citep[in these called
\enquote{rate of
  transmission}]{shannon1948,kelly1956}[\sect~14.7]{pressetal1988_r2007}[\chap~2]{coveretal1991_r2006}:
\begin{equation}
  \label{eq:pre_mutual_info_new_ind}
  \begin{split}
  \mI\bigl( \ysi{0} : \ygi{0} \| \yD,\yI \bigr) &\defd
  \sH\bigl(\yso \| \yD,\yI\bigr) -
  \sH\bigl(\yso \| \ygo; \yD,\yI)
  \\
  &\equiv
  \sum_{\mathclap{\ysi{0},\ygi{0}}} \pf\bigl(\ysi{0}, \ygi{0} \| \yD, \yI\bigr)
 \, \ln\frac{\pf(\ysi{0}, \ygi{0} \| \yD, \yI)}{
    \pf(\ysi{0} \| \yD, \yI)\;
    \pf(\ygi{0} \| \yD, \yI)}
  \\
  &\equiv
  \sum_{\mathclap{\ysi{0},\ygi{0}}} \pf\bigl(\ysi{0}, \ygi{0} \| \yD, \yI\bigr)
 \, \ln\frac{\pf(\ysi{0}\| \ygi{0}, \yD, \yI)}{
    \pf(\ysi{0} \| \yD, \yI)}.
\end{split}
\end{equation}
The second expression shows that the mutual information also measures the
discrepancy between our \dob\ about the variates jointly and the product of
our \dobs\ about them separately; the third expression shows that it is an
average value of the log-ratio between our \dobs\ about the insomnia
symptoms conditioned and unconditioned on the genetic data. A mutual
information of around $0.1\;\textrm{nats}$ means that the two \dobs\
roughly differ in their second significant digit; generally
$10^{-d}\;\textrm{nats}$ means a difference in the $(d+1)$th significant
digit. The mutual information vanishes if the symptoms and genetic
variations are completely irrelevant to one another, and is equal to the
entropy $\sH\bigl(\yso \|
\yD,\yI\bigr)$ %, $\sH\bigl(\ygo \| \yD,\yI\bigr)$
if knowledge of the gene variate gives us complete certainty about the
insomnia symptoms, since $\sH\bigl(\yso \| \ygo; \yD,\yI)$ vanishes in this
case.


The mutual information depends on the knowledge on which our \dob\ is
based, in this case the data and initial knowledge $\yD\yI$. We'll
obviously consider initial states of knowledge $\yI$ such that
\begin{equation}
\mI\bigl( \ysi{0} : \ygi{0} \| \yI \bigr) =0,
\end{equation}
that is, we assume no a priori relevance of one variate upon the other.
This kind of initial information $\yI$ can still strongly emphasize or
de-emphasize the effect of the data on our knowledge when the latter are
few compared to the range of the variates.


\medskip

To calculate the mutual information~\eqref{eq:pre_mutual_info_new_ind} given
the data we need the joint \dobs~\eqref{eq:pre_goal_ind_plausibilities}. To
calculate the latter we use \eqn~\eqref{eq:pre_joint_prob_new_ind_F_unknown},
which needs our \dobs\ about the joint frequency
distribution~\eqref{eq:pre_goal_plausibilities}. These can be calculated from
our initial \dobs\ $\pf( \yF \| \yI)$ via Bayes's theorem:
\begin{equation}
  \label{eq:pre_F_from_f_generic}
  \pf(\yF \| \yf, n, \yI) =
  \frac{\pf(\yf \| n,\yF, \yI)\; \pf(\yF \| \yI)}{
  \sum_{\yF}\pf(\yf \| n,\yF, \yI)\; \pf(\yF \| \yI)}.
\end{equation}

Bayes's theorem requires our \dobs\ about the joint frequency distribution
$\yf$ of the variates in the sampled population, given the distribution
$\yF$ in the full population. This problem is similar to \enquote{drawing
  from an urn without replacement}, for which our \dobs\ are represented by
the multivariate hypergeometric distribution:
\begin{equation}
  \label{eq:pre_multivariate_hyperg}
  \pf(\yf \| n,\yF, \yI) =
  \binom{N}{n}^{-1}\,\yprod\binom{N\yF}{n\yf}
  \equiv \binom{N}{N\yF}^{-1}\,\binom{n}{n\yf}\,\binom{N-n}{N\yF-n\yf}
\end{equation}
\citep{ghoshetal1997}[parts~I, VI]{freedmanetal1978_r2007}[summaries
in][\chap~8]{gelmanetal1995_r2014}[\chap~3]{jaynes1994_r2003}[properties of
this distribution are discussed
in][\sect~4.8.3]{ross1976_r2010}[\sect~II.6]{feller1950_r1968}. For $N$
very large this distribution simplifies to a multinomial one:
\begin{equation}
  \label{eq:pre_multivariate_hyperg_Ninf}
  \pf(\yf \| n,\yF, \yI) =\binom{n}{n\yf} \;\yprod \yF^{n\yf}.
\end{equation}

Combining \eqns~\eqref{eq:pre_joint_prob_new_ind_F_unknown},
\eqref{eq:pre_F_from_f_generic}, \eqref{eq:pre_multivariate_hyperg}, and
simplifying we obtain
\begin{equation}
  \label{eq:pre_joint_prob_new_ind_full}
  \pf\bigl(\yso, \ygo \| \yf, n,\yI\bigr)  =
  \frac{
    \sum_{\yF} X_{\yso, \ygo}\;
    \pf(\yF \| \yI)\;\yprod\binom{N\yF}{n\yf}}{
  \sum_{\yF} \pf(\yF \| \yI)\;\yprod\binom{N\yF}{n\yf}}.
\end{equation}
With an analogous reasoning we find analogous formulae for
$\pf\bigl(\yso \| \yf, n,\yI\bigr)$ by replacing $\yF$, $\yf$ with
$\yFs$, $\yfs$; and for $\pf\bigl(\ygo \| \yf, n,\yI\bigr)$ by
replacing $\yF$, $\yf$ with $\yFg$, $\yfg$. From these we can calculate the
mutual information~\eqref{eq:pre_mutual_info_new_ind}.
If $N$ is very large the formula above becomes
\begin{equation}
  \label{eq:pre_joint_prob_new_ind_full_Ninf}
  \pf\bigl(\yso, \ygo \| \yf, n,\yI\bigr)  =
  \frac{
    \int X_{\yso, \ygo}\;\bigl( \yprod\yF^{n\yf} \bigr)\;
    \pf(\yF \| \yI)\;\di\yF}{
  \int\bigl( \yprod\yF^{n\yf} \bigr)\; \pf(\yF \| \yI)\;\di\yF}.
\end{equation}

\subsection{Motivation for the infinite-population limit}
\label{sec:pre_motivation_Ninf}

In the previous sections we have considered the cases of finite and
infinite $N$. There are two reasons for focusing on the infinite-$N$ case.

First: we are interested in a general connection between
insomnia symptoms and gene alleles, independent of the size of the full
population. Thus we can consider our sample as coming from a hypothetical
population of statistically similar biological characteristics.

Second: if the number of alleles $l\gg\log_{2}N$, then every individual
will have a unique combination of alleles, leading to \dobs\
\eqref{eq:pre_joint_prob_new_ind_full} about the frequencies $\yFg$ and $\yF$
that are either zero or unity, since each combination appears only once or
not at all. This in turn leads to a mutual information with maximal value.
But the reason of this maximal value is in this case not biological, but
purely statistical. The way to bypass this is to consider an infinite
population size. We will still have to take care of this statistical
phenomenon in what follows.

\subsection{Calculation: Dirichlet distributions}
\label{sec:pre_2nd_calculation_unif_marginals}

\mynote{The results of this section will be improved using the analysis by
  Good \citey[\chaps~4--5]{good1965}{good1980}}

%\mynote{\textbf{IMPORTANT: check \citep{zhangetal2012b} and its refs in \sect~2}}

We need to assess our initial \dob\ for $\yF$ according to some background
knowledge. We'd also like our initial \dob\ to have a convenient
mathematical expression.

We choose to express initial state of knowledge $\yIo$ with a Dirichlet
distribution \mynote{refs}, which depends on a positive parameter $\yA$ and
a non-negative $C$-tuple $\ya$ such that $\ysum\ya=1$, which we take equal
to $(1/C,\dotsc,1/C)$:
\begin{equation}
  \label{eq:pre_dirichlet_prior}
  \pf(\yF \| \ya, \yA,\yIo)\,\di\yF =
  \frac{\Gamma(\yA)}{\yprod\Gamma(\yA\ya)}\;
  \yprod \yF^{\yA\ya-1}\;\di\yF,\qquad
  \ya \defd (\underbrace{1/C,\dotsc,1/C}_{\text{$C$ elements}}).
\end{equation}
Our choice is motivated by the following reasons.

First, this distribution
is uniquely determined by the assumption, called \enquote{sufficientness}
\citep{zabell1982,dinizetal2016}\mynote{more refs}, that only the data
about a specific variate value, say $(\yso, \ygo)$ are relevant for our
belief about the frequency $X_{\yso, \ygo}$ of that value:
\begin{equation}
  \label{eq:pre_sufficientness}
  \pf(X_{\yso, \ygo} \| \yf, n, \ya, \yA,\yIo\bigr)\,\di\yF \equiv
  \pf(X_{\yso, \ygo} \| x_{\yso,\ygo}, n, \ya, \yA,\yIo\bigr)\,\di\yF.
\end{equation}
This in particular implies that our \dobs\ about the insomnia symptoms or
any particular gene allele are not affected by the number of alleles we're
considering. That is, if we decide to consider additional genes, our
inferences are consistent with the ones previously made when those genes
were not considered; likewise if we decide to neglect some genes.

Second, the Dirichlet distribution is the conjugate of the multinomial
distribution~\eqref{eq:pre_multivariate_hyperg_Ninf}, which means that our
\dob\ updated from the data will still be expressed by a Dirichlet
distribution but with new parameters $\yA'$, $\ya'$:
\begin{multline}
  \label{eq:pre_update_dirichlet_prior}
  \pf(\yF \| \yf, n, \ya, \yA,\yIo)\,\di\yF =
  \frac{\Gamma(\yA')}{\yprod\Gamma(\yA'\ya')}\;
  \yprod \yF^{\yA'\ya'-1}\;\di\yF,\\
  \text{with}\qquad
  \yA' \defd n + \yA,
  \quad
  \ya' \defd \frac{n\yf + \yA\ya}{n+\yA}.
\end{multline}

This distribution has mean and covariance matrix
\begin{equation}\label{eq:pre_mean_covariance_dirichlet}
  \E(\yF \| \yA, \ya,\yIo) = \ya, \qquad
  \cov(\yF \| \yA, \ya,\yIo) =
  \frac{1}{\yA+1}(\diag\ya -\ya\otimes\ya).
\end{equation}
Equations~\eqref{eq:pre_update_dirichlet_prior} and
\eqref{eq:pre_mean_covariance_dirichlet} show the meanings of the parameters
$\ya$, $\yA$:




\mynote{Rest of the section has to be changed from here on}
Let's preliminarily consider background
knowledge $\yIc$ such that the \dob\ is constant for all possible
distributions $\yF$. There are $\binom{N+C-1}{C-1}$ possible distributions
$\yF$; hence
\begin{equation}
  \label{eq:pre_constant_prior}
  \pf(\yF \| \yIc) = \binom{N+C-1}{C-1}^{-1}.
\end{equation}
This yields a constant  initial  distribution of \dob\ for $(\yso,\ygo)$:
\begin{multline}
  \label{eq:pre_joint_prob_initial_F_unknown}
  \pf\bigl(\yso, \ygo \| \yIc\bigr)  =
  \sum_{\yF}  \pf\bigl(\yso, \ygo \| \yF,\yIc\bigr) \;
  \pf( \yF \| \yIc) ={}\\
  \sum_{\yF}  X_{\yso, \ygo} \binom{N+C-1}{C-1}^{-1}
  = 1/C,
\end{multline}

This state of knowledge is denoted with \enquote{$\gn$} because it depends
on the number of gene variations we consider. Two different values $\gn'$
and $\gn''$ correspond to two different states of knowledge,
$\yI_{\gn'} \ne \yI_{\gn''}$. It's important to note that these states of
knowledge yield different initial \dobs\ for the marginal frequency
distribution of a single gene. As a consequence, if we suddenly decide to
consider additional genes, or to neglect some, we cannot compare our
inference with the ones previously made.

Another bothering feature of the state of knowledge $\yIc$ is that the
marginal \dob\ in the insomnia symptoms alone given the full set of
data, \eqn~\eqref{eq:pre_marginals_final_const_prior}, depends on the number of
gene variations considered in the data. Likewise, the \dob\ in the
gene variations depend on the number of insomnia symptoms.

It seems reasonable to consider an initial state of knowledge that doesn't
lead to different marginal distributions of \dobs\ in the frequencies when
we want to consider an additional gene variation, and that doesn't have
the counter-intuitive features above.

One such state of knowledge $\yIo$ exists (perhaps it isn't unique) and is
characterized by a Dirichlet-multinomial distribution for $\yF$, which
depends on a positive parameter $\yA$ and a non-negative $C$-tuple $\ya$
such that $\ysum\ya=1$, which we take equal to $(1/C,\dotsc,1/C)$:
\begin{multline}
  \label{eq:pre_dirichlet_prior}
  \pf(\yF \| \yA, \ya,\yIo) =
  \binom{N+\yA-1}{N}^{-1} \yprod\binom{N\yF+\yA\ya-1}{N\yF},\\
  \ya \defd (\underbrace{1/C,\dotsc,1/C}_{\text{$C$ elements}})
\end{multline}
\citep[\sect~13.1]{johnsonetal1969_r1996}[\sect~3]{minka2000_r2012}[and
especially][\sects~3--4]{basuetal1982}.

This distribution has mean and covariance matrix
% \begin{equation}\label{eq:pre_mean_covariance_dirichlet}
%   \E(\yF \| \yA, \ya,\yIo) = \ya, \qquad
%   \cov(\yF \| \yA, \ya,\yIo) =
%   \frac{\yA+N}{N\,(\yA+1)}(\diag\ya -\ya\otimes\ya).
% \end{equation}
It also has the property of leaving the distribution of \dob\ for the
marginal frequencies of any number of gene variations invariant if we
consider more genes \citep[\sects~3--4]{basuetal1982}, provided $\yA$ is
kept fixed, with no dependence on $C$ for example. This parameter
determines our \dob\ about the frequency of single gene variations, for
example for the first gene allele, $G_{\yg_0}$: it is a beta-binomial
distribution
\begin{equation}
  \label{eq:pre_dirichlet_prior_marginal_beta-binomial}
  \pf(G_{\yg_0} \| \yA, \ya,\yIo) =
  \binom{N+\yA-1}{N}^{-1} \yprod\binom{N G_{\yg_0}+\yA/2-1}{N G_{\yg_0}}.
\end{equation}

The parameter $\yA$ represents the strength of our belief that $\yF$ is a
uniform distribution. It can be considered as a prior number of
observations in which we found that the $C$ values of the joint variate
came all in equal proportions. Its value -- in particularly in comparison
with the sample size $n$ -- can therefore greatly influence
our inferences. We consider two particular values:
\begin{itemize}[label=--]
\item If $\yA=2$ then our \dob\ about the frequency of each gene allele is
  uniform; it doesn't assign equal \dobs\ to the possible frequency
  distributions for the $\yCs\equiv 8$ symptom combinations; % but rather
  % \begin{multline}
  %   \label{eq:pre_dirichlet_prior_symptoms}
  %   \pf(\yFs \| \yIo)\,\di\yFs = \frac{1}{\yprod\Gamma\bigl(
  %   \frac{2}{\yCs} \bigr)}
  %   \yprod \yFs^{\frac{2}{\yCs}-1}\,\delt(\ysum\yFs-1)\,\di\yFs ={}\\
  %   \frac{1}{8\Gamma\bigl( \frac{1}{4} \bigr)} \yprod
  %   \yFs^{-3/4}\,\delt(\ysum\yFs-1)\,\di\yFs.
  % \end{multline}
  but the density of \dob\ for the frequency of controls vs cases is
  uniform.
\item If $\yA$ is large, of the order of millions or more, then our \dob\
  about the joint frequency distribution $\yF$ of all variates is uniform
  -- but no longer uniform for the marginal frequencies.
\end{itemize}


% The approximation $N \to \infty$ needs to be justified. If we consider a
% limited amount of gene variations, so that $N/C$ is large (say, less than
% 12 gene variations), this approximation should be valid. If we consider a
% large amount of gene variations, so that $C/N$ is large (say, more than 25
% gene variations), the possible frequencies lie on the faces of the
% $(C-1)$-dimensional simplex; but the density~\eqref{eq:pre_dirichlet_prior} is
% concentrated in regions at the boundary of the simplex, and thus the
% approximation may still be reasonable.

As shown by Basu \amp\ de Bragan\c{c}a Pereira \citey[\sect~4,
Theorem~2]{basuetal1982}, the initial \dob\ $\yIo$ leads to a \dob\ for
$N\yF-n\yf$ conditional on our data $\yD$ that has the same form, but with
new parameters $n+\yA$ and $(n\yf+\yA\ya)/(n+\yA)$:
\begin{equation}
  \label{eq:pre_dirichlet_posterior_X}
  \pf(N\yF -n\yf \| \yf, n, \yA, \ya,\yIo) =
  \binom{N+\yA-1}{N-n}^{-1} \yprod\binom{N\yF+\yA\ya-1}{N\yF-n\yf},
\end{equation}
from which we can deduce the \dob~\eqref{eq:pre_F_from_f_generic}, using
\eqref{eq:pre_multivariate_hyperg}. Moreover, the expectation of $N\yF-n\yf$ is
\citey[\sect~3]{basuetal1982}
\begin{equation}
  \label{eq:pre_dirichlet_posterior_expectation}
  \E(N\yF -n\yf \| \yf, n, \yA, \ya,\yIo) =
  (N-n) \frac{n\yf+\yA\ya}{n+\yA},
\end{equation}
from which we find
\begin{equation}
    \label{eq:pre_expect_freq_dirichlet_prior_final}
    \E(\yF \| \yf, n,\yA,\ya, \yIo)  =
    \frac{n}{N}x_{\yso, \ygo} + \frac{N-n}{N}
\frac{n x_{\yso, \ygo}+\yA/C}{n+\yA}.
\end{equation}
This expression can be combined with
\eqn~\eqref{eq:pre_joint_prob_new_ind_F_unknown} to finally find
\begin{equation}
    \label{eq:pre_joint_prob_new_ind_full_dirichlet_prior_final}
    \pf(\yso, \ygo \| \yf, n,\yA,\ya, \yIo)  =
    \frac{n}{N}x_{\yso, \ygo} + \frac{N-n}{N}
\frac{n x_{\yso, \ygo}+\yA/C}{n+\yA}.
\end{equation}

From equation~\eqref{eq:pre_mean_covariance_dirichlet} we can calculate the
variance of our \dob\ in the frequency of a particular variate value:
\begin{multline}
  \label{eq:pre_variance_single_gene_dirichlet}
  \var(X_{\ys,\yg} \| \yf, n,\yA,\ya, \yIo)  ={}\\
  \frac{N+n+\yA}{N\,(n+\yA+1)}
  \frac{n x_{\yso, \ygo}+\yA/C}{n+\yA}
  \Biggl(1- \frac{n x_{\yso, \ygo}+\yA/C}{n+\yA} \Biggr).
\end{multline}


\subsection*{Calculation: Dirichlet distributions with finite $N$}
%\label{sec:pre_2nd_calculation_unif_marginals}

\mynote{Old section, kept for reference}

%\mynote{\textbf{IMPORTANT: check \citep{zhangetal2012b} and its refs in \sect~2}}

We need to assess our initial \dob\ for $\yF$ according to some
background knowledge. Let's preliminarily consider background
knowledge $\yIc$ such that the \dob\ is constant for all possible
distributions $\yF$. There are $\binom{N+C-1}{C-1}$ possible distributions
$\yF$; hence
\begin{equation}
  \label{eq:pre_NO-constant_prior}
  \pf(\yF \| \yIc) = \binom{N+C-1}{C-1}^{-1}.
\end{equation}
This yields a constant  initial  distribution of \dob\ for $(\yso,\ygo)$:
\begin{multline}
  \label{eq:pre_NO-joint_prob_initial_F_unknown}
  \pf\bigl(\yso, \ygo \| \yIc\bigr)  =
  \sum_{\yF}  \pf\bigl(\yso, \ygo \| \yF,\yIc\bigr) \;
  \pf( \yF \| \yIc) ={}\\
  \sum_{\yF}  X_{\yso, \ygo} \binom{N+C-1}{C-1}^{-1}
  = 1/C,
\end{multline}

This state of knowledge is denoted with \enquote{$\gn$} because it depends
on the number of gene variations we consider. Two different values $\gn'$
and $\gn''$ correspond to two different states of knowledge,
$\yI_{\gn'} \ne \yI_{\gn''}$. It's important to note that these states of
knowledge yield different initial \dobs\ for the marginal frequency
distribution of a single gene. As a consequence, if we suddenly decide to
consider additional genes, or to neglect some, we cannot compare our
inference with the ones previously made.

Another bothering feature of the state of knowledge $\yIc$ is that the
marginal \dob\ in the insomnia symptoms alone given the full set of
data, \eqn~\eqref{eq:pre_NO-marginals_final_const_prior}, depends on the number of
gene variations considered in the data. Likewise, the \dob\ in the
gene variations depend on the number of insomnia symptoms.

It seems reasonable to consider an initial state of knowledge that doesn't
lead to different marginal distributions of \dobs\ in the frequencies when
we want to consider an additional gene variation, and that doesn't have
the counter-intuitive features above.

One such state of knowledge $\yIo$ exists (perhaps it isn't unique) and is
characterized by a Dirichlet-multinomial distribution for $\yF$, which
depends on a positive parameter $\yA$ and a non-negative $C$-tuple $\ya$
such that $\ysum\ya=1$, which we take equal to $(1/C,\dotsc,1/C)$:
\begin{multline}
  \label{eq:pre_NO-dirichlet_prior}
  \pf(\yF \| \yA, \ya,\yIo) =
  \binom{N+\yA-1}{N}^{-1} \yprod\binom{N\yF+\yA\ya-1}{N\yF},\\
  \ya \defd (\underbrace{1/C,\dotsc,1/C}_{\text{$C$ elements}})
\end{multline}
\citep[\sect~13.1]{johnsonetal1969_r1996}[\sect~3]{minka2000_r2012}[and
especially][\sects~3--4]{basuetal1982}.

This distribution has mean and covariance matrix
\begin{equation}\label{eq:pre_NO-mean_covariance_dirichlet}
  \E(\yF \| \yA, \ya,\yIo) = \ya, \qquad
  \cov(\yF \| \yA, \ya,\yIo) =
  \frac{\yA+N}{N\,(\yA+1)}(\diag\ya -\ya\otimes\ya).
\end{equation}
It also has the property of leaving the distribution of \dob\ for the
marginal frequencies of any number of gene variations invariant if we
consider more genes \citep[\sects~3--4]{basuetal1982}, provided $\yA$ is
kept fixed, with no dependence on $C$ for example. This parameter
determines our \dob\ about the frequency of single gene variations, for
example for the first gene allele, $G_{\yg_0}$: it is a beta-binomial
distribution
\begin{equation}
  \label{eq:pre_NO-dirichlet_prior_marginal_beta-binomial}
  \pf(G_{\yg_0} \| \yA, \ya,\yIo) =
  \binom{N+\yA-1}{N}^{-1} \yprod\binom{N G_{\yg_0}+\yA/2-1}{N G_{\yg_0}}.
\end{equation}

The parameter $\yA$ represents the strength of our belief that $\yF$ is a
uniform distribution. It can be considered as a prior number of
observations in which we found that the $C$ values of the joint variate
came all in equal proportions. Its value -- in particularly in comparison
with the sample size $n$ -- can therefore greatly influence
our inferences. We consider two particular values:
\begin{itemize}[label=--]
\item If $\yA=2$ then our \dob\ about the frequency of each gene allele is
  uniform; it doesn't assign equal \dobs\ to the possible frequency
  distributions for the $\yCs\equiv 8$ symptom combinations; % but rather
  % \begin{multline}
  %   \label{eq:pre_NO-dirichlet_prior_symptoms}
  %   \pf(\yFs \| \yIo)\,\di\yFs = \frac{1}{\yprod\Gamma\bigl(
  %   \frac{2}{\yCs} \bigr)}
  %   \yprod \yFs^{\frac{2}{\yCs}-1}\,\delt(\ysum\yFs-1)\,\di\yFs ={}\\
  %   \frac{1}{8\Gamma\bigl( \frac{1}{4} \bigr)} \yprod
  %   \yFs^{-3/4}\,\delt(\ysum\yFs-1)\,\di\yFs.
  % \end{multline}
  but the density of \dob\ for the frequency of controls vs cases is
  uniform.
\item If $\yA$ is large, of the order of millions or more, then our \dob\
  about the joint frequency distribution $\yF$ of all variates is uniform
  -- but no longer uniform for the marginal frequencies.
\end{itemize}


% The approximation $N \to \infty$ needs to be justified. If we consider a
% limited amount of gene variations, so that $N/C$ is large (say, less than
% 12 gene variations), this approximation should be valid. If we consider a
% large amount of gene variations, so that $C/N$ is large (say, more than 25
% gene variations), the possible frequencies lie on the faces of the
% $(C-1)$-dimensional simplex; but the density~\eqref{eq:pre_NO-dirichlet_prior} is
% concentrated in regions at the boundary of the simplex, and thus the
% approximation may still be reasonable.

As shown by Basu \amp\ de Bragan\c{c}a Pereira \citey[\sect~4,
Theorem~2]{basuetal1982}, the initial \dob\ $\yIo$ leads to a \dob\ for
$N\yF-n\yf$ conditional on our data $\yD$ that has the same form, but with
new parameters $n+\yA$ and $(n\yf+\yA\ya)/(n+\yA)$:
\begin{equation}
  \label{eq:pre_NO-dirichlet_posterior_X}
  \pf(N\yF -n\yf \| \yf, n, \yA, \ya,\yIo) =
  \binom{N+\yA-1}{N-n}^{-1} \yprod\binom{N\yF+\yA\ya-1}{N\yF-n\yf},
\end{equation}
from which we can deduce the \dob~\eqref{eq:pre_NO-F_from_f_generic}, using
\eqref{eq:pre_NO-multivariate_hyperg}. Moreover, the expectation of $N\yF-n\yf$ is
\citey[\sect~3]{basuetal1982}
\begin{equation}
  \label{eq:pre_NO-dirichlet_posterior_expectation}
  \E(N\yF -n\yf \| \yf, n, \yA, \ya,\yIo) =
  (N-n) \frac{n\yf+\yA\ya}{n+\yA},
\end{equation}
from which we find
\begin{equation}
    \label{eq:pre_NO-expect_freq_dirichlet_prior_final}
    \E(\yF \| \yf, n,\yA,\ya, \yIo)  =
    \frac{n}{N}x_{\yso, \ygo} + \frac{N-n}{N}
\frac{n x_{\yso, \ygo}+\yA/C}{n+\yA}.
\end{equation}
This expression can be combined with
\eqn~\eqref{eq:pre_NO-joint_prob_new_ind_F_unknown} to finally find
\begin{equation}
    \label{eq:pre_NO-joint_prob_new_ind_full_dirichlet_prior_final}
    \pf(\yso, \ygo \| \yf, n,\yA,\ya, \yIo)  =
    \frac{n}{N}x_{\yso, \ygo} + \frac{N-n}{N}
\frac{n x_{\yso, \ygo}+\yA/C}{n+\yA}.
\end{equation}

From equation~\eqref{eq:pre_NO-mean_covariance_dirichlet} we can calculate the
variance of our \dob\ in the frequency of a particular variate value:
\begin{multline}
  \label{eq:pre_NO-variance_single_gene_dirichlet}
  \var(X_{\ys,\yg} \| \yf, n,\yA,\ya, \yIo)  ={}\\
  \frac{N+n+\yA}{N\,(n+\yA+1)}
  \frac{n x_{\yso, \ygo}+\yA/C}{n+\yA}
  \Biggl(1- \frac{n x_{\yso, \ygo}+\yA/C}{n+\yA} \Biggr).
\end{multline}

\subsection*{Constant initial \dob}
\label{sec:pre_first_calc_const_prior}
\mynote{This section has become irrelevant. Left here just for reference}

We need to assess our initial \dob\ for $\yF$ according to some
background knowledge. As a first tentative let's consider background
knowledge $\yIc$ such that the \dob\ is constant for all possible
distributions $\yF$. There are $\binom{N+C-1}{C-1}$ possible distributions
$\yF$; hence
\begin{equation}
  \label{eq:pre_constant_prior}
  \pf(\yF \| \yIc) = \binom{N+C-1}{C-1}^{-1}.
\end{equation}
This yields a constant  initial  distribution of \dob\ for $(\yso,\ygo)$:
\begin{multline}
  \label{eq:pre_joint_prob_initial_F_unknown}
  \pf\bigl(\yso, \ygo \| \yIc\bigr)  =
  \sum_{\yF}  \pf\bigl(\yso, \ygo \| \yF,\yIc\bigr) \;
  \pf( \yF \| \yIc) ={}\\
  \sum_{\yF}  X_{\yso, \ygo} \binom{N+C-1}{C-1}^{-1}
  = 1/C,
\end{multline}
because the last sum is a convex combination, with equal weights, of all
points in a simplex of dimension $C-1$, giving its centre of mass
$(1/C, 1/C, \dotso)$.

With the initial knowledge $\yIc$,
\eqn~\eqref{eq:pre_joint_prob_new_ind_full} simplifies to
\begin{equation}
    \label{eq:pre_joint_prob_new_ind_full_simplified}
  \pf\bigl(\yso, \ygo \| \yf, n,\yIc\bigr)  =
  \frac{
    \sum_{\yF} X_{\yso, \ygo}\,
    \yprod\binom{N\yF}{n\yf}}{
  \sum_{\yF} \yprod\binom{N\yF}{n\yf}}.
\end{equation}
The sum in the denominator can be calculated with an identity for
multinomial coefficients\mynote{add
  refs for summation formula}:
\begin{equation}
  \label{eq:pre_multinomial_sum_identity}
  \sum_{\yF} \yprod\binom{N\yF}{n\yf} = \binom{N+C-1}{n+C-1},
\end{equation}
which, substituted in \eqn~\eqref{eq:pre_F_from_f_generic}, also leads to
\begin{equation}
  \label{eq:pre_F_from_f_specific}
  \pf(\yF \| \yf, n, \yIc) = \binom{N+C-1}{n+C-1}^{-1}\,\yprod\binom{N\yF}{n\yf}.
\end{equation}

The sum in the numerator of
\eqn~\eqref{eq:pre_joint_prob_new_ind_full_simplified} can be
rewritten\mynote{explain the steps} obtaining
\begin{equation}
  \label{eq:pre_numerator_constant_prior}
  \sum_{\yF} X_{\yso, \ygo}\, \yprod\binom{N\yF}{n\yf}
  =
  \frac{n x_{\yso, \ygo}+1}{N} \binom{N+C}{n+C}
  - \frac{1}{N}\binom{N+C-1}{n+C-1}.
\end{equation}

Simplifying we finally find
\begin{equation}
    \label{eq:pre_joint_prob_new_ind_full_const_prior_final}
  \pf\bigl(\yso, \ygo \| \yf, n,\yIc\bigr)  =
\frac{(N+C) n x_{\yso, \ygo}+N-n}{N\,(n+C)}
\end{equation}
This expression can be interpreted as a weighted sum of the observed
frequency $x_{\yso,\ygo}$ in the data and the initial \dob\ $1/C$,
\eqn~\eqref{eq:pre_joint_prob_initial_F_unknown}:
\begin{equation}
    \label{eq:pre_joint_prob_new_ind_full_const_prior_final_reinterpret}
  \pf\bigl(\yso, \ygo \| \yf, n,\yIc\bigr)  \propto
x_{\yso, \ygo}+ \frac{N-n}{n}\frac{C}{N+C}\,\frac{1}{C},
\end{equation}
the ratio of the second to the first weight being
$(N-n)/n\times C/(N+C)$\mynote{Luca: I don't find this intuitively
  satisfying, though I don't know why. It'd be good to try another initial
  state of knowledge}. When $n=N$, that is, when we've sampled the full
population, the second weight is zero and we're left with $x_{\yso,\ygo}$,
which is also equal to $X_{\yso,\ygo}$, consistent with
\eqn~\eqref{eq:pre_joint_prob_new_ind_F_known}.

For large $C$ this is approximately
\begin{equation}
    \label{eq:pre_joint_prob_new_ind_full_const_prior_final_approx_C}
    \pf\bigl(\yso, \ygo \| \yf, n,\yIc\bigr)  \approx
    \frac{n x_{\yso, \ygo}}{N} +
    \frac{(N-n)\,(n x_{\yso, \ygo} +1)}{N C}
\end{equation}
and for large $N$
\begin{equation}
    \label{eq:pre_joint_prob_new_ind_full_const_prior_final_approx_N}
    \pf\bigl(\yso, \ygo \| \yf, n,\yIc\bigr)  \approx
    \frac{n x_{\yso, \ygo} +1}{n + C}.
\end{equation}


For the marginal distributions we obtain, by summation,
\begin{gather}
  \label{eq:pre_marginals_final_const_prior}
    \pf\bigl(\yso \| \yf, n,\yIc\bigr)  =
\frac{(N+C) n s_{\yso}+(N-n)\yCg}{N\,(n+C)},
\\
    \pf\bigl(\ygo \| \yf, n,\yIc\bigr)  =
\frac{(N+C) n g_{\ygo}+(N-n)\yCs}{N\,(n+C)}.
\end{gather}
Note that if the initial distribution of \dob\ about the joint frequencies
$\yF$ is uniform, then the \dobs\ for the marginal frequencies $\yFs$ and
$\yFg$ are \emph{not} uniform.

Using the
distributions~\eqref{eq:pre_joint_prob_new_ind_full_const_prior_final}
and~\eqref{eq:pre_marginals_final_const_prior} in the formula for the mutual
information~\eqref{eq:pre_mutual_info_new_ind} and simplifying we find
\begin{multline}
  \label{eq:pre_mutual_info_final}
  \mI\bigl( \yso : \ygo \| \yD,\yIc \bigr) =
  \ln[N\,(n+C)] +
  \sum_{\mathclap{\yso,\ygo}}
  \frac{(N+C) n x_{\yso, \ygo}+N-n}{N\,(n+C)}\times{}\\
  \ln\frac{(N+C) n x_{\yso, \ygo}+N-n}{
    [(N+C) n s_{\yso}+(N-n)\yCg]\,
    [(N+C) n g_{\ygo}+(N-n)\yCs]
  }
\end{multline}

We can divide this sum into two parts: one sum over the values of $\yso$
and $\ygo$ which appear in our data, for which $g_{\ygo}>0$; and one sum
over the $\yCgn$ remaining $\ygo$ values,  for which $g_{\ygo}=x_{\yso,\ygo}=0$:
\begin{multline}
  \label{eq:pre_mutual_info_final_three}
  \mI\bigl( \yso : \ygo \| \yD,\yIc \bigr) =
  \ln[N\,(n+C)] \\
  \begin{aligned}[b]
  &+
    \!\begin{multlined}[t][0.92\linewidth]
\sum_{\yso}
\sum_{\mathclap{\ygo \in \yD}}
  \frac{(N+C) n x_{\yso, \ygo}+N-n}{N\,(n+C)}\times{}\\
  \ln\frac{(N+C) n x_{\yso, \ygo}+N-n}{
    [(N+C) n s_{\yso}+(N-n)\yCg]\,
    [(N+C) n g_{\ygo}+(N-n)\yCs]
  } 
\end{multlined}
    \\
    &-\yCgn\,
      \frac{N-n}{N\,(n+C)}\sum_{\yso}\ln\{\yCs\,[(N+C) n s_{\yso}+(N-n)\yCg]\}
  \end{aligned}
\end{multline}

Our initial information is that each individual is characterized by an
insomnia parameter in $\set{1,\dotsc,8}$ and $94$ gene allele pairs, for a
total of $8 \times 2^{94} \approx 1.6 \times 10^{29}$ possible combined
values. The Norwegian or European populations, denoted $N$, are roughly
$5.3 \times 10^6$ and $740 \times 10^6$, so it's impossible for all values
to be present in the population. From a mathematical point of view, the set
of possible frequency distributions of the values reside in a
$10^6$--$10^8$-dimensional subspace at the edges of the
$10^{29}$-dimensional simplex of frequency distributions. From this point
of view the limit $N\to \infty$ does not make much sense. Unless we choose
a prior  distribution of \dob\ for the frequency distributions that is
concentrated on the edges of the simplex.

\bigskip
\hrule
[Luca's memoranda:]
\begin{itemize}
\item Use of partial exchangeability \emph{has to} distinguish also between
  men and women: see Gehrman \etal\ \citey[p.~327]{gehrmanetal2013}.
\item This study could also be used to detect most relevant genes, by
  eliminating them in turn (and in pairs etc) and checking the ensuing
  predictions.
\item Is it computationally possible to use a \enquote{nonparametric
    model}? It would avoid unwarranted assumptions and phenomena like
  overtranining.
\end{itemize}

%

%\setlength{\intextsep}{0.5ex}% with wrapfigure
%\begin{figure}[p!]%{r}{0.4\linewidth} % with wrapfigure
%  \centering\includegraphics[trim={12ex 0 18ex 0},clip,width=\linewidth]{maxent_saddle.png}\\
%\caption{***}\label{fig:comparison_a5}
%\end{figure}% exp_family_maxent.nb

%\newpage
\iffalse
\begin{acknowledgements}
  PGLPM thanks Mari \amp\ Miri for continuous encouragement and affection, and
  to Buster Keaton and Saitama for filling life with awe and inspiration.
  To the developers and maintainers of \LaTeX, Emacs, AUC\TeX, Open Science
  Framework, Python, Inkscape, Sci-Hub for making a free and unfiltered
  scientific exchange possible.
%\rotatebox{15}{P}\rotatebox{5}{I}\rotatebox{-10}{P}\rotatebox{10}{\reflectbox{P}}\rotatebox{-5}{O}.
%\sourceatright{\autanet}
\end{acknowledgements}
\fi


%%%%%%%%%%%%%%% BIB %%%%%%%%%%%%%%%

\defbibnote{prenote}{{\footnotesize (\enquote{de $X$} is listed under D,
    \enquote{van $X$} under V, and so on, regardless of national
    conventions.)\par}}
% \defbibnote{postnote}{\par\medskip\noindent{\footnotesize% Note:
%     \arxivp \mparcp \philscip \biorxivp}}

\printbibliography[prenote=prenote%,postnote=postnote
]


\end{document}

---------- cut text ----------------
% cluster, no-parallel, 25 genes: user=4264, elapsed=4943
% cluster, 20 cores, 25 genes: user=34, elapsed=664
% cluster, 20 cores, 26 genes: user=38, elapsed=792
% cluster, 20 cores, 26 genes: user=42, elapsed=950
% cluster, 20 cores, 65 genes: user=59, elapsed=1898

% c(6, 9, 17, 21, 30, 31, 32, 34, 38, 42, 46, 51, 53, 57, 58, 59, 61, 64, 66, 68, 69, 70, 72, 75, 78, 83, 88, 89, 90, 92, 93) give  0.00234867116380749

% c(6, 32, 57, 59, 64, 68, 72, 88, 89, 90) give  0.0791343853227863 (4% normalized)

% c(59, 68, 88, 89, 90) give 0.0245318392903188 (1.3% normalized)

% c(6, 9, 21, 31, 32, 34, 46, 53, 57, 58, 59, 61, 64, 66, 68, 69, 72, 88,
% 89, 90) give 0.00113696780949303 (0.05% normalized)



%%% Local Variables: 
%%% mode: LaTeX
%%% TeX-PDF-mode: t
%%% TeX-master: t
%%% End: 
